focal_prop_perID = matrix(NA,n_simulations,group_size)  #obsevred proportion of time engaged in behavior X per ID
focal_rate_perID = matrix(NA,n_simulations,group_size)  #observed rate of behavior per ID
# behav_timeObserved_focal_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
# behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
if (any(is.nan(scan_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_rate_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_prop_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
y_max = max(c(max(plot_scan$y), max(plot_focal$y), max(plot_true$y)))
#Plot results
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_rate_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Rate per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true proportion per individual
geom_density(aes(true_prop_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_prop_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_prop_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_prop_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
y_max
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
summary(studentratings)
packages()
update.packages()
update.packages(ask=FALSE)
library(ggplot2)
library(furrr)
source("Simulation Scripts/simulation_functions.r")
#source("~/Documents/Github/rethinking-obs-methods/Simulation Scripts/simulation_functions.r")
set.seed(1234)
source("~/Documents/Github/rethinking-obs-methods/Simulation Scripts/simulation_functions.r")
seq(30, 180, by = 5)
simulation_parameters <-
list(
n_days = seq(30, 180, by = 5),
# number of observation days to simulate (assume 7h per day, see below)
group_size = seq(10, 100, by = 5),
p_terrain_visibility = seq(0.1, 1, by = 0.1),
p_behavior_visibility = seq(0.1, 1, by = 0.1),
mean_events = c(seq(1, 19, by = 1),
seq(20, 50, by = 5)),
# mean number of behavioral events per day, per individual (sd set as mean/3, see below)
behavior_duration = c(seq(1, 9, by = 1),
seq(10, 55, by = 5),
seq(60, 110, by = 10),
seq(120, 600, by = 60)),
# behavior duration in sec
focal_duration_min = c(seq(5, 25, by = 5),
seq(30, 60, by = 10)),
# time of focal observation in minutes
focal_break_time_min = 5,
# minimum break time between focals in minutes
scan_obsTime_perID = seq(1, 11, by = 2),
# scan time needed per individual in seconds
scan_break_time_min = c(seq(5, 25, by = 5),
seq(30, 60, by = 10))
) # minimum break time between end of scan and start of new one in seconds
i = 1
lapply(simulation_parameters, sample, 1)
n_events <-
round(# if not given, calculate average number of daily interactions per individual for this simulation run
abs(
rnorm(
# assuming normal distribution of events per individual
sim_values$group_size[i],
# for each individual in the group pick a number of daily interactions from a normal distribution with
sim_values$mean_events[i],
# mean_events as set in simulations_parameters and
2
)
)) + 1 # sd set as mean_events/3, a reasonable variation that will keep a similar distribution for different means
# add 1 because sometimes there would be 0s and then it cracks
print(paste(c(sim_values, sample(1:1000, 1)), collapse = '_'))
i = 1
sim_values <- lapply(simulation_parameters, sample, 1)
n_events <-
round(# if not given, calculate average number of daily interactions per individual for this simulation run
abs(
rnorm(
# assuming normal distribution of events per individual
sim_values$group_size[i],
# for each individual in the group pick a number of daily interactions from a normal distribution with
sim_values$mean_events[i],
# mean_events as set in simulations_parameters and
2
)
)) + 1 # sd set as mean_events/3, a reasonable variation that will keep a similar distribution for different means
# add 1 because sometimes there would be 0s and then it cracks
print(paste(c(sim_values, sample(1:1000, 1)), collapse = '_'))
behavior_duration = 10
n_hours = 7*90
time_in_s <-
n_hours * 60 * 60 # number of seconds of observations in a study period
beh_time_seq <- seq.int(behavior_duration, floor(time_in_s - behavior_duration),
by = behavior_duration)
#ReactionNorm_Survival.R
#Model proximity network with bison - get a distribution of possible networks from the observed data
#Regression from imputed dataset to test the change in proximity over time
#C. Testard October 2022
#Load libraries
#data wrangling
library(dplyr)
library(forcats)
#Modelling
library(lme4)
library(broom.mixed)
#For plotting
library(ggplot2)
library(sjPlot)
#Load data
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/")
load('Survival_Adults_TimeVarying_allgroups.RData')
load("BisonProximity.RData")
#Set group year list
group = c("F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V","F")
years = c(2013, 2013,2014,2014,2015,2015,2015,2015,
2016,2016,2016,2016,2017,2017,2017,
2018,2018, 2019, 2019,2021,2021,2022,2022,2022)
groupyears = paste0(group,years)
######################################
## STEP 1: CREATE IMPUTED DATASSET ###
######################################
# Extract change in proximity from each draw to create imputed data set:
imputed.data=list(); imputed.data.prepost=list(); i=1; gy=1;
imputed.density=list();
for (i in 1:num_iter){
data.all<-data.frame(); density.iter=data.frame(matrix(NA, nrow=num_iter)); names(density.iter)="dens"
density.all<-data.frame();
for (gy in 1:length(groupyears)){
#Extract behavioral data
data_path<-'~/Documents/GitHub/Cayo-Maria-Survival/Data/Data All Cleaned/BehavioralDataFiles/'
data = read.csv(paste0(data_path,"Group",groupyears[gy],"_GroupByYear.txt")) #load meta data
data = data[data$hrs.focalfollowed>0,]
data$group = group[gy]
data$year= years[gy];
data$isPost = ifelse(years[gy]<2018, "pre","post")
data$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
data$id = as.factor(data$id)
data$sex[data$sex=="FALSE"]="F"; data$sex = as.factor(data$sex);
data$id.year = paste(data$id, data$year,sep='.')
strength<-node_strength_all[[groupyears[gy]]][i,]
degree<-node_degree_all[[groupyears[gy]]][i,]
node.id<-node_ids[[groupyears[gy]]]
data$prox.strength<-as.numeric(strength[match(data$id, node.id)])
data$prox.degree<-as.numeric(degree[match(data$id, node.id)])
#data.merged<-merge(data,data.survival[,c("id.year","Age_entry.days","Age_event.days",
#                                    "Survival","period")], by="id.year")
data.final<-data[,c("id","sex","age","group","year","isPost","isPost.year",
"id.year","prox.strength", "prox.degree")]
data.all<-rbind(data.all, data.final)
density.iter$dens=density_all[[groupyears[gy]]]
density.iter$group=group[gy]
density.iter$year=years[gy]
density.iter$isPost = ifelse(years[gy]<2018, "pre","post")
density.iter$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
density.all<-rbind(density.all, density.iter)
}
#Add death status info
SurvivalData.ALL$age_at_end = SurvivalData.ALL$DOD-SurvivalData.ALL$DOB
SurvivalData.ALL$age_at_end[is.na(SurvivalData.ALL$age_at_end)] = as.Date("2023-01-01")-SurvivalData.ALL$DOB[is.na(SurvivalData.ALL$age_at_end)]
dead.ids = unique(SurvivalData.ALL$id[which(!is.na(SurvivalData.ALL$YearOfDeath))])
data.all$status=0
data.all$status[!is.na(match(data.all$id, dead.ids))]=1; #data.all$status=as.factor(data.all$status)
data.all$age_at_end = as.numeric(SurvivalData.ALL$age_at_end[match(data.all$id, SurvivalData.ALL$id)])
#Standardize
density.all$std.dens <-(density.all$dens-mean(density.all$dens))/sd(density.all$dens)
data.all$std.prox.strength = (data.all$prox.strength-mean(data.all$prox.strength))/sd(data.all$prox.strength)
data.all$std.prox.degree = (data.all$prox.degree-mean(data.all$prox.degree))/sd(data.all$prox.degree)
#Create year factor
data.all$year.factor = as.factor(data.all$year)
#Adjust levels
data.all$group<-as.factor(data.all$group)
data.all = data.all %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
mutate(isPost.year = fct_relevel(isPost.year,
"pre", "post.2018","post.2019","post.2021", "post.2022"))
#"post.2018","pre","post.2019","post.2021", "post.2022"))
density.all = density.all %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
mutate(isPost.year = fct_relevel(isPost.year,
"pre", "post.2018","post.2019","post.2021", "post.2022"))
#"post.2018","pre","post.2019","post.2021", "post.2022"))
data.all <- data.all %>%
group_by(id) %>%
mutate(status = ifelse(row_number() == 1, status, NA))
data.all$status=as.factor(data.all$status)
imputed.data[[i]]=data.all
imputed.density[[i]]=density.all
#If only consider IDs present both pre and post
id.isPost = table(data.all$id, data.all$isPost)
id.year = table(data.all$id, data.all$year);
id_pre = row.names(as.data.frame(which(id.isPost[,"pre"]>0))); id_post = row.names(as.data.frame(which(id.isPost[,"post"]>0)))
id.PreAndPost = as.data.frame(row.names(as.data.frame(which(id.isPost[,"pre"]>0 & id.isPost[,"post"]>0)))); names(id.PreAndPost)="id"
id.PreAndPost$group = data.all$group[match(id.PreAndPost$id, data.all$id)]
table(id.PreAndPost$group)
data.prepost = subset(data.all, id %in% id.PreAndPost$id)
imputed.data.prepost[[i]]<-data.prepost
print(i)
}
#Create imputed dataset to use in frequentist survival models
imp<-miceadds::datalist2mids(imputed.data)
imp.prepost<-miceadds::datalist2mids(imputed.data.prepost)
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]
save(imputed.data.prepost, imputed.data, "ReactionNormAnalysis_data.RData")
save(imputed.data.prepost, imputed.data, file="ReactionNormAnalysis_data.RData")
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]
model<-lmer(std.prox.strength~  1+ isPost+ scale(age)+ sex + (1|group) + (1+ isPost|id), data=data)
plot(model)
qqnorm(residuals(model))
summary(model)
augment(model)%>% select(id,sex,isPost,.fitted,std.prox.strength)%>%
filter(sex=="F")%>% gather(type,std.prox.strength, `.fitted`:std.prox.strength)%>%
ggplot(.,aes(x=isPost,y=std.prox.strength,group=id))+
geom_line(alpha=0.3)+
theme_classic()+ facet_grid(.~type)
#For modeling
library(MCMCglmm)
library(tidyverse)
library(broom)
#library(nadiv)
library(gridExtra)
library(lattice)
augment(model)%>% select(id,sex,isPost,.fitted,std.prox.strength)%>%
filter(sex=="F")%>% gather(type,std.prox.strength, `.fitted`:std.prox.strength)%>%
ggplot(.,aes(x=isPost,y=std.prox.strength,group=id))+
geom_line(alpha=0.3)+
theme_classic()+ facet_grid(.~type)
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[2]]
model<-lmer(std.prox.strength~  1+ isPost+ scale(age)+ sex + (1|group) + (1+ isPost|id), data=data)
plot(model)
qqnorm(residuals(model))
augment(model)%>% select(id,sex,isPost,.fitted,std.prox.strength)%>%
filter(sex=="F")%>% gather(type,std.prox.strength, `.fitted`:std.prox.strength)%>%
ggplot(.,aes(x=isPost,y=std.prox.strength,group=id))+
geom_line(alpha=0.3)+
theme_classic()+ facet_grid(.~type)
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]
model<-lmer(std.prox.strength~  1+ isPost+ scale(age)+ sex + (1|group) + (1+ isPost|id), data=data)
plot(model)
qqnorm(residuals(model))
summary(model)
augment(model)%>% select(id,sex,isPost,.fitted,std.prox.strength)%>%
filter(sex=="F")%>% gather(type,std.prox.strength, `.fitted`:std.prox.strength)%>%
ggplot(.,aes(x=isPost,y=std.prox.strength,group=id))+
geom_line(alpha=0.3)+
theme_classic()+ facet_grid(.~type)
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]; data$status=as.numeric(data$status)
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]; data$status=as.numeric(data$status)-1
#Extract reaction norm by taking the random effect
data = imputed.data.prepost[[1]]; data$status=as.numeric(data$status)-1
View(data)
model<-lmer(std.prox.strength~  1+ isPost+ scale(age)+ sex + (1|group) + (1+ isPost|id), data=data)
prior_biv_RR_px <-list(R= list(V =diag(c(1,0.0001),2,2),nu=0.002,fix=2),
G=list(G1=list(V= matrix(c(1,0,0, 0,1,0, 0,0,1),3,3, byrow=TRUE),
nu=3, alpha.mu=rep(0,3), alpha.V=diag(25^2,3,3))))
mcmc_biv_RR<-MCMCglmm(cbind(std.prox.strength,
status)~trait-1+
at.level(trait,1):isPost+
at.level(trait,1):scale(age,scale=FALSE)+
at.level(trait,1):sex+
at.level(trait,1):group,
random=~ us(trait+isPost:at.level(trait,1)):id,
rcov=~ idh(trait):units, family=c("gaussian","gaussian"),
prior=prior_biv_RR_px,
nitt=950000,
burnin=50000, thin=450,
verbose=TRUE, data= as.data.frame(data),
pr=TRUE, saveX=TRUE,saveZ=TRUE)
#load data
setwd("~/Documents/GitHub/Cayo-Maria-Survival/ReactionNormTutorial/")
df_plast<-read_csv("aggression.csv")
data$sex.num = 0; data$sex.num[data$sex=="F"]=1
data$sex.num = 0; data$sex.num[data$sex=="F"]=1
data$group.num=as.numeric(data$group)
prior_biv_RR_px <-list(R= list(V =diag(c(1,0.0001),2,2),nu=0.002,fix=2),
G=list(G1=list(V= matrix(c(1,0,0, 0,1,0, 0,0,1),3,3, byrow=TRUE),
nu=3, alpha.mu=rep(0,3), alpha.V=diag(25^2,3,3))))
mcmc_biv_RR<-MCMCglmm(cbind(std.prox.strength,
status)~trait-1+
at.level(trait,1):isPost+
at.level(trait,1):scale(age,scale=FALSE)+
at.level(trait,1):sex.num+
at.level(trait,1):group.num,
random=~ us(trait+isPost:at.level(trait,1)):id,
rcov=~ idh(trait):units, family=c("gaussian","gaussian"),
prior=prior_biv_RR_px,
nitt=950000,
burnin=50000, thin=450,
verbose=TRUE, data= as.data.frame(data),
pr=TRUE, saveX=TRUE,saveZ=TRUE)
debug(MCMCglmm:::priorformat)
data$isPost.num=as.numeric(data$isPost)
mcmc_biv_RR<-MCMCglmm(cbind(std.prox.strength,
status)~trait-1+
at.level(trait,1):isPost.num+
at.level(trait,1):scale(age,scale=FALSE)+
at.level(trait,1):sex.num+
at.level(trait,1):group.num,
random=~ us(trait+isPost.num:at.level(trait,1)):id,
rcov=~ idh(trait):units, family=c("gaussian","gaussian"),
prior=prior_biv_RR_px,
nitt=950000,
burnin=50000, thin=450,
verbose=TRUE, data= as.data.frame(data),
pr=TRUE, saveX=TRUE,saveZ=TRUE)
is.null(prior)
mcmc_biv_RR<-MCMCglmm(cbind(std.prox.strength,
status)~trait-1+
at.level(trait,1):isPost+
at.level(trait,1):scale(age,scale=FALSE)+
at.level(trait,1):sex,#+
#at.level(trait,1):group,
random=~ us(trait+isPost:at.level(trait,1)):id,
rcov=~ idh(trait):units, family=c("gaussian","gaussian"),
#prior=prior_biv_RR_px,
nitt=950000,
burnin=50000, thin=450,
verbose=TRUE, data= as.data.frame(data),
pr=TRUE, saveX=TRUE,saveZ=TRUE)
c
Q
mcmc_biv_RR<-MCMCglmm(cbind(std.prox.strength,
status)~trait-1+
at.level(trait,1):isPost+
at.level(trait,1):scale(age,scale=FALSE)+
at.level(trait,1):sex,#+
#at.level(trait,1):group,
random=~ us(trait+isPost:at.level(trait,1)):id,
rcov=~ idh(trait):units, family=c("gaussian","gaussian"),
#prior=prior_biv_RR_px,
nitt=950000,
burnin=50000, thin=450,
verbose=TRUE, data= as.data.frame(data),
pr=TRUE, saveX=TRUE,saveZ=TRUE)
