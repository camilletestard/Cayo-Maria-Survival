---
title: "Building proximity networks using bison"
author: "Camille Testard"
date: "11/04/2021"
output: html_document
---


# to check with Jordan:


```{r setup, include=FALSE}
# empty global environment 
rm(list=ls())

# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)

#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Code/Bayesian_code_bison/")
source("scripts/functions_GlobalNetworkMetrics.R")
```
# Create edgelist - format from cayo data

```{r, message=FALSE}

group= "V"
year=2016
groupyear = paste(group,year,sep="")

#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))

if (groupyear == "V2019"){ #quick fix for now
  prox_idx = !is.na(prox_data$partners.activity..sequential.) #find indices where there are individuals in proximity
  #add focal monkey in proximity column
  prox_data$in.proximity[prox_idx]= paste(prox_data$focal.monkey[prox_idx], 
                                          prox_data$in.proximity[prox_idx],sep=",")
}

#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id#[meta_data$focalcutoff_met=="Y"]
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")

#extract the number of unique IDs
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA


#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))

df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)] 
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration 
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples

## Add id qualifiers
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names); 
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"

head(df_obs_agg)

save(df_obs_agg,file = "Proximity_df.RData")
#NOTES: 
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.

```

```
# set priors
set weakly informative (almost flat) prior that assumes rates can up to very high rates/hour (a true flat prior would go up to infinity), with only slightly higher probability of 0
more informative priors influence too much when data is rather sparce
priors are set in log space, so that rates can only be positive

```

```{r}
#Set gamma priors

prior_alpha <- 1
prior_beta <- 1

# Check prior for edge weights (rate parameter)
plot(density(rgamma(1e5, prior_alpha, prior_beta)))


```

# run posterior predictive check 2
plot BISoN edge weight estimates against point edge weight estimates (counts/observation time or number of samples)

```{r}

num_dyads <- nrow(df_obs_agg)
num_samples <- 1000
posterior_samples <- matrix(0, num_dyads, num_samples)

for (i in 1:num_dyads) {
  posterior_samples[i, ] <- rgamma(num_samples, prior_alpha + df_obs_agg$count[i], prior_beta + df_obs_agg$total_obs_time[i])
}

posterior_quantiles <- apply(posterior_samples, 1, function(x) quantile(x, probs=c(0.5, 0.05, 0.95)))

point_edge_weights <- df_obs_agg$count/df_obs_agg$total_obs_time
plot(point_edge_weights, posterior_quantiles[1, ], xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(point_edge_weights, posterior_quantiles[2, ], point_edge_weights, posterior_quantiles[3, ], angle=0)
abline(a=0, b=1)

# the model never will model true zero's because it can never be certain a dyad never interacts (absence of evidence)
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)

# BISoN is slightly overestimating zeroes compared to point estimates (or the point estimates might be underestimating the edge weights) 
# similar to what mentioned above: this is because BISoN will never estimate tre zero's 
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
```

# extract edge weights
dyadic edge weights are now distributions which can be extracted as 
- a matrix with one column per dyad & one row per sample
- a distribution of adjacency matrices corresponding single draw of the posterior adjacency matrices 
  (stored in a 'tensor': a multidimensional matrix)
- an edgelist with uncertainty (i.e. 2.5% & 97.5% CI)

```{r}
######################################################
# matrix with one row per dyad & one column per sample
rownames(posterior_samples) <- df_obs_agg$dyad_id

name_matrix <- paste0("matrices_edgeWeights/", groupyear, "_edge_weight_samples.csv")

write.csv(as.data.frame(posterior_samples), name_matrix) 

######################################################
# distribution of adjacency matrices 
adj_tensor <- array(0, c(nr_ind, nr_ind, num_samples)) # create empty multidimensional matrix
#ISSUE: the edgelist does not have entries for individuals with themselves

for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
  adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- posterior_samples[i,1:num_samples]
}
# View(adj_tensor[, , 1]) # see how 1 of adjacency those matrixes looks like

name_tensor <- paste0("tensor_AdjacencyMatrices/", groupyear, "_adjacency_matrices.rds")

saveRDS(adj_tensor, name_tensor)

######################################################
# edgelist with 2.5% & 97.5% CI
prob_edgelist <- t(posterior_quantiles)
colnames(prob_edgelist) <- c("50%", "2.5%", "97.5%")
prob_edgelist <- as.data.frame(prob_edgelist)
prob_edgelist <- cbind(dyad = substring(rownames(prob_edgelist),8), prob_edgelist)

name_edgelist <- paste0("edgelist_uncertainty/", groupyear, "_edgelist_with_uncertainty.csv")

write_csv(prob_edgelist, name_edgelist) 
```

# visualise uncertainty
to plot a network with a visualisation of the uncertainty around edge weights add a semi-transparent line around each edge with a width that corresponds to the uncertainty measure (the normalised difference between the 97.5% and 2.5% credible interval estimate for each edge weigh) to do this, generate two igraph objects (one for the main network & one for the uncertainty in edges) and plot them with the same coordinates, so they appear on top of each other

```{r, fig.width=8}
# calculate lower, median, and upper quantiles of edge weights
adj_quantiles <- apply(adj_tensor, c(1, 2), function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
adj_lower <- adj_quantiles[1, , ] # 2.5% credible interval
adj_mid <- adj_quantiles[2, , ]
adj_upper <- adj_quantiles[3, , ] # 97.5% credible interval
adj_range <- (adj_upper - adj_lower)

# set threshold for minimum edge weight of edges to be shown on network
threshold <- 0.15

# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges 
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error around edges

# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3,vertex.label=NULL) 
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25), 
     vertex.label=unique_names, vertex.size=3,
     vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
```

# calculate network metrics
now that the edges are probabilities, incorporate that uncertainty further down into calculations of network metrics

```{r}

#####################
## NETWORK DENSITY ##

# calculating weighted density for the whole network 
dens <- rep(0, nr_samples) # create empty vector 
for (i in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
  s <- sum(adj_tensor[, , i])
  dens[i] <- s/nr_dyads 
}

# plot the distribution of the network density values
plot(density(dens), xlab="network density")

# save posterior draws of network density to use in further analyses
name_network_density <- paste0("Net_densities/", groupyear, "_posterior_network_density.csv")
write_csv(data.frame(draws=dens), name_network_density)

##########################
## Node weighted degree ##

w.degree <- matrix(0, nr_ind, nr_samples) # create empty vector 
row.names(w.degree)=unique_names;
for (s in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
  g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
  w.degree[,s] <- strength(g)
}


name_network_density <- paste0("Weighted_degree/", groupyear, "_weighted_degree.csv")
write_csv(data.frame(draws=w.degree), name_network_density)

```

# save workspace

```{r}
# save.image("~/Documents/MACAQUE R PACKAGE/BISoN/2. edgelists to BISoN.RData")
```
