log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_obs_time)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
nr_samples <- 10000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix
inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution
for (i in 1:nr_samples) { # fill in matrix
log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
# sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model
#cbind(row.names(summary(fit_inla)$fixed), levels(df_obs_agg$dyad_id))
jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)
plot(
df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd),
rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) + rnorm(nr_dyads, sd=jitter_sd),
col=rgb(0, 0, 1, jitter_sd),
xlab="Observed event counts",
ylab="Predicted event counts"
)
for (i in 1:20) {
points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) +
rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)
rates_bison
df_obs_agg$count
c(df_obs_agg$count, rates_bison)
cbind(df_obs_agg$count, rates_bison)
cbind(df_obs_agg$dyad_id, df_obs_agg$count, rates_bison)
cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison)
rm(list=ls())
# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Bayesian_code_bison/")
#source("scripts/simulations.R")
source("scripts/sampler.R")
source("scripts/functions_GlobalNetworkMetrics.R")
group= "V"
year=2016
groupyear = paste(group,year,sep="")
#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))
if (groupyear == "V2019"){ #quick fix for now
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)]=
paste(prox_data$focal.monkey[!is.na(prox_data$partners.activity..sequential.)],
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)],sep=",")
}
#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")
#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))
df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)]
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples
## Add id qualifiers
df_obs_agg$ID1 = as.factor(df_obs_agg$ID1); df_obs_agg$ID2 = as.factor(df_obs_agg$ID2);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"
head(df_obs_agg)
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA
log_rate_mu <- 0
log_rate_sigma <- 1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_obs_time)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
nr_samples <- 10000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix
inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution
for (i in 1:nr_samples) { # fill in matrix
log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
# sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model
cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison)
jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)
plot(
df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd),
rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) + rnorm(nr_dyads, sd=jitter_sd),
col=rgb(0, 0, 1, jitter_sd),
xlab="Observed event counts",
ylab="Predicted event counts"
)
for (i in 1:20) {
points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) +
rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)
rates_bison_lower <- exp(summary(fit_inla)$fixed[1:nr_dyads, 3])
rates_bison_upper <- exp(summary(fit_inla)$fixed[1:nr_dyads, 5])
rates_point_est <- df_obs_agg$count/df_obs_agg$total_obs_time
y_min <- min(rates_bison_lower)
y_max <- max(rates_bison_upper)
plot(rates_point_est, rates_bison, col="#387780", ylim=c(y_min, y_max), xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(rates_point_est, rates_bison_lower, rates_point_est, rates_bison_upper, length=0)
abline(a=0, b=1)
# empty global environment
rm(list=ls())
# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Bayesian_code_bison/")
#source("scripts/simulations.R")
source("scripts/sampler.R")
source("scripts/functions_GlobalNetworkMetrics.R")
group= "V"
year=2016
groupyear = paste(group,year,sep="")
#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))
if (groupyear == "V2019"){ #quick fix for now
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)]=
paste(prox_data$focal.monkey[!is.na(prox_data$partners.activity..sequential.)],
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)],sep=",")
}
#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")
#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))
df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)]
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples
## Add id qualifiers
df_obs_agg$ID1 = as.factor(df_obs_agg$ID1); df_obs_agg$ID2 = as.factor(df_obs_agg$ID2);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"
head(df_obs_agg)
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA
save(df_obs_agg,file = "Proximity_df.RData")
#NOTES:
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.
log_rate_mu <- 0
log_rate_sigma <- 2
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_obs_time)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
nr_samples <- 10000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix
inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution
for (i in 1:nr_samples) { # fill in matrix
log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
# sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model
#cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison) #Used to check the ordering of dyads
jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)
plot(
df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd),
rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) + rnorm(nr_dyads, sd=jitter_sd),
col=rgb(0, 0, 1, jitter_sd),
xlab="Observed event counts",
ylab="Predicted event counts"
)
for (i in 1:20) {
points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_obs_time) +
rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)
# the model never will model true zero's because it can never be certain a dyad never interacts (absence of evidence)
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
rates_bison_lower <- exp(summary(fit_inla)$fixed[1:nr_dyads, 3])
rates_bison_upper <- exp(summary(fit_inla)$fixed[1:nr_dyads, 5])
rates_point_est <- df_obs_agg$count/df_obs_agg$total_obs_time
y_min <- min(rates_bison_lower)
y_max <- max(rates_bison_upper)
plot(rates_point_est, rates_bison, col="#387780", ylim=c(y_min, y_max), xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(rates_point_est, rates_bison_lower, rates_point_est, rates_bison_upper, length=0)
abline(a=0, b=1)
# BISoN is slightly overestimating zeroes compared to point estimates (or the point estimates might be underestimating the edge weights)
# similar to what mentioned above: this is because BISoN will never estimate tre zero's
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
rates_samples <- exp(log_rates_samples) # exponentiating the log_p samples to get back at the real values
rates_samples <- exp(log_rate_samples) # exponentiating the log_p samples to get back at the real values
colnames(rates_samples) <- df_obs_agg$dyad_id
name_matrix <- paste0("matrices_edgeWeights/", name_dataset, "edge_weight_samples.csv")
name_matrix <- paste0("matrices_edgeWeights/", groupyear, "_edge_weight_samples.csv")
name_matrix
write_csv(as.data.frame(rates_samples), name_matrix)
i=1
df_obs_agg[i,"ID2_id"][[1]]
df_obs_agg[i,"ID1_id"][[1]]
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
# View(adj_tensor[, , 1]) # see how 1 of adjacency those matrixes looks like
name_tensor <- paste0("tensor_AdjacencyMatrices/", groupyear, "adjecency_matrices.rds")
saveRDS(adj_tensor, name_tensor)
prob_edgelist <- round(exp(summary(fit_inla)$fixed[, c(1, 3, 5)]), 2)
colnames(prob_edgelist) <- c("50%", "2.5%", "97.5%")
prob_edgelist <- as.data.frame(prob_edgelist)
prob_edgelist <- cbind(dyad = substring(rownames(prob_edgelist),5), prob_edgelist)
prob_edgelist <- round(exp(summary(fit_inla)$fixed[, c(1, 3, 5)]), 2)
colnames(prob_edgelist) <- c("50%", "2.5%", "97.5%")
prob_edgelist <- as.data.frame(prob_edgelist)
prob_edgelist <- cbind(dyad = substring(rownames(prob_edgelist),5), prob_edgelist)
View(prob_edgelist)
rownames(prob_edgelist)
substring(rownames(prob_edgelist),7)
rownames(prob_edgelist),8)
substring(rownames(prob_edgelist),8)
prob_edgelist <- round(exp(summary(fit_inla)$fixed[, c(1, 3, 5)]), 2)
colnames(prob_edgelist) <- c("50%", "2.5%", "97.5%")
prob_edgelist <- as.data.frame(prob_edgelist)
prob_edgelist <- cbind(dyad = substring(rownames(prob_edgelist),8), prob_edgelist)
View(prob_edgelist)
write_csv(prob_edgelist, name_edgelist)
name_edgelist <- paste0("edgelist_uncertainty/", name_dataset, "edgelist_with_uncertainty.csv")
name_edgelist <- paste0("edgelist_uncertainty/", groupyear, "_edgelist_with_uncertainty.csv")
adj_quantiles <- apply(adj_tensor, c(1, 2), function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
adj_lower <- adj_quantiles[1, , ] # 2.5% credible interval
adj_mid <- adj_quantiles[2, , ]
adj_upper <- adj_quantiles[3, , ] # 97.5% credible interval
adj_range <- (adj_upper - adj_lower)
threshold <- 0.1
# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error around edges
# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
threshold <- 0.5
# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error
# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
threshold <- 0.1
# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error around edges
# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
adj_quantiles <- apply(adj_tensor, c(1, 2), function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
adj_lower <- adj_quantiles[1, , ] # 2.5% credible interval
adj_mid <- adj_quantiles[2, , ]
adj_upper <- adj_quantiles[3, , ] # 97.5% credible interval
adj_range <- (adj_upper - adj_lower)
threshold <- 0.2
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
threshold <- 0.15
# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error around edges
# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3,vertex.label=NULL)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
# calculating weighted density
dens <- rep(0, nr_samples) # create empty vector
for (i in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
s <- sum(adj_tensor[, , i])
dens[i] <- s/nr_dyads
}
dens <- rep(0, nr_samples) # create empty vector
for (i in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
s <- sum(adj_tensor[, , i])
dens[i] <- s/nr_dyads
}
# plot the distribution of the network density values
plot(density(dens), xlab="network density")
adj_tensor[, , i]
name_network_density <- paste0("Net_densities/", groupyear, "posterior_network_density.csv")
write_csv(data.frame(draws=dens), name_network_density)
test=data.frame(draws=dens)
View(test)
strength(adj_tensor[, , i])
g = graph.adjacency(adj_tensor[, , i],mode= "undirected",weighted=T)
strength(g)
w.degree <- rep(0, nr_ind, nr_samples) # create empty vector
for (s in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
w.degree[,s] <- strength(g)
}
strength(g)
w.degree[,s]
w.degree <- rep(0, nr_ind, nr_samples) # create empty vector
g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
w.degree[,s] <- strength(g)
w.degree <- matrix(0, nr_ind, nr_samples) # create empty vector
for (s in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
w.degree[,s] <- strength(g)
}
View(df_obs_agg)
# distribution of adjacency matrices
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2"][[1]], df_obs_agg[i,"ID1"][[1]],] <- rates_samples[1:nr_samples, i]
}
# View(adj_tensor[, , 1]) # see how 1 of adjacency those matrixes looks like
name_tensor <- paste0("tensor_AdjacencyMatrices/", groupyear, "_adjecency_matrices.rds")
saveRDS(adj_tensor, name_tensor)
adj_tensor[, , i]
View(df_obs_agg)
i = 1
df_obs_agg[i,"ID2"][[1]]
df_obs_agg[i,"ID1"][[1]]
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2"][[1]], df_obs_agg[i,"ID1"][[1]],] <- rates_samples[1:nr_samples, i]
}
adj_tensor[,,1]
row.names(adj_tensore)
row.names(adj_tensor)
row.names(adj_tensor[,,1])
rownames(adj_tensor[,,1])
View(df_obs_agg)
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
rownames(adj_tensor[,,1])
adj_tensor[,,1]
View(df_obs_agg)
df_obs_agg[i,"ID2"][[1]]
colnames(adj_tensor)[df_obs_agg[i,"ID2"][[1]]]
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional
unique(df_obs_agg$ID1)
as.character(unique(df_obs_agg$ID1))
length(as.character(unique(df_obs_agg$ID1)))
cbind(as.character(unique(df_obs_agg$ID1)), as.character(unique(df_obs_agg$ID2))
)
adj_tensor[,,1]
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
#ISSUE: the edgelist does not have entries for individuals with themselves
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
# V
adj_tensor[,,1]
diag(adj_tensor[,,1])
df_obs_agg[,1:2]
df_obs_agg[,c(1,2)]
df_obs_agg[,c("ID1","ID2")]
weightedEL = df_obs_agg[,c("ID1","ID2")]
View(weightedEL)
weightedEL$weight = rates_samples[i,]
View(weightedEL)
dils::AdjacencyFromEdgelist(weightedEL)
adj_tensor[,,i] <- dils::AdjacencyFromEdgelist(weightedEL)
test = dils::AdjacencyFromEdgelist(weightedEL)
adjmat<- dils::AdjacencyFromEdgelist(weightedEL)
adj_tensor[,,i] <- adjMat[["adjacency"]]; rownames(adj_tensor[,,i]) = adjMat[["nodelist"]]; colnames(adj_tensor[,,i]) = adjMat[["nodelist"]]
adj_tensor[,,i] <- adjmat[["adjacency"]]; rownames(adj_tensor[,,i]) = adjmat[["nodelist"]]; colnames(adj_tensor[,,i]) = adjmat[["nodelist"]]
adj_tensor[,,i]
diag(adj_tensor[,,i])
rownames(adj_tensor[,,i])
adjmat[["nodelist"]]
weightedEL
View(weightedEL)
df_obs_agg[i,"ID2_id"][[1]]
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
diag(adj_tensor[,,1])
max(df_obs_agg$ID2_id)
adjmat[["nodelist"]]
adjmat
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2"][[1]], df_obs_agg[i,"ID1"][[1]],] <- rates_samples[1:nr_samples, i]
}
diag(adj_tensor[,,1])
edgelist <- read_tsv("~/Documents/Github/Cayo-Maria-Survival/Bayesian_code_bison/mulatta_brent_F_2012_affi_groom_NA_frequency_edgelist.txt")
edgelist[i,"ID1_id"][[1]]
df_obs_agg$ID1_id-df_obs_agg$ID2_id
df_obs_agg$ID1_id==df_obs_agg$ID2_id
df_obs_agg$dyad_id[df_obs_agg$ID1_id==df_obs_agg$ID2_id]
df_obs_agg$dyad_id[df_obs_agg$ID1==df_obs_agg$ID2]
edgelist$ID1 = as.factor(edgelist$ID1)
edgelist$ID2 = as.factor(edgelist$ID2)
edgelist$ID1==edgelist$ID2
df_obs_agg$ID1 = as.factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = as.factor(df_obs_agg$ID2, levels = unique_names);
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because
df_obs_agg$ID1 = as.factor(df_obs_agg$ID1, levels = unique_names)
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names)
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2"][[1]], df_obs_agg[i,"ID1"][[1]],] <- rates_samples[1:nr_samples, i]
}
diag(adj_tensor[,,1])
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
diag(adj_tensor[,,1])
df_obs_agg$ID1_id-df_obs_agg$ID2_id
df_obs_agg$ID1_id==df_obs_agg$ID2_id
which(df_obs_agg$ID1_id==df_obs_agg$ID2_id)
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
#ISSUE: the edgelist does not have entries for individuals with themselves
for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
diag(adj_tensor[,,1])
name_tensor <- paste0("tensor_AdjacencyMatrices/", groupyear, "_adjecency_matrices.rds")
saveRDS(adj_tensor, name_tensor)
