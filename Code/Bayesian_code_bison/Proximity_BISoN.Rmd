---
title: "Building proximity networks using bison"
author: "Camille Testard"
date: "11/04/2021"
output: html_document
---


# to check with Jordan:


```{r setup, include=FALSE}
# empty global environment 
rm(list=ls())

# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)

#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Code/Bayesian_code_bison/")
source("scripts/functions_GlobalNetworkMetrics.R")
```
# Create edgelist - format from cayo data

```{r, message=FALSE}

group= "V"
year=2016
groupyear = paste(group,year,sep="")

#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))

if (groupyear == "V2019"){ #quick fix for now
  prox_idx = !is.na(prox_data$partners.activity..sequential.) #find indices where there are individuals in proximity
  #add focal monkey in proximity column
  prox_data$in.proximity[prox_idx]= paste(prox_data$focal.monkey[prox_idx], 
                                          prox_data$in.proximity[prox_idx],sep=",")
}

#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id#[meta_data$focalcutoff_met=="Y"]
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")

#extract the number of unique IDs
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA


#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))

df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)] 
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration 
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples

## Add id qualifiers
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names); 
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"

head(df_obs_agg)

save(df_obs_agg,file = "Proximity_df.RData")
#NOTES: 
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.

```

```
# set priors
set weakly informative (almost flat) prior that assumes rates can up to very high rates/hour (a true flat prior would go up to infinity), with only slightly higher probability of 0
more informative priors influence too much when data is rather sparce
priors are set in log space, so that rates can only be positive

```

```{r}
#PROBLEM: PRIORS NEED TO BE CHANGED.
#gamma conjugate.

#IMPORTANT NOTE: these priors are calibrated for rates of events over HOURS of observation, not number of scan samples.

#Priors for number of events per hours observed.
log_rate_mu <- 0
log_rate_sigma <-1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually

#Priors for number of events per scan samples:

```
# fit the edge weight model

```{r}
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors 
# note that in INLA it uses precision instead of SD (which is 1/SD)

#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
  count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
  family="poisson", # poisson for frequency/count data
  data=df_obs_agg,
  control.fixed=prior.fixed,
  control.compute=list(config = TRUE)
)

```

# get INLA model output in matrix form 
with 1 column per dyad & then 10000 samples of the edge weights

```{r}
nr_samples <- 1000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix

inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution 

for (i in 1:nr_samples) { # fill in matrix
  log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
   # sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
```

# run posterior predictive check 1
plot predicted event counts against observed event counts

```{r}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model

#cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison) #Used to check the ordering of dyads

jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)

plot(
  df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), 
  rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) + rnorm(nr_dyads, sd=jitter_sd), 
  col=rgb(0, 0, 1, jitter_sd),
  xlab="Observed event counts",
  ylab="Predicted event counts"
)

for (i in 1:20) {
  points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) + rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)

# the model never will model true zero's because it can never be certain a dyad never interacts (absence of evidence)
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
```
# run posterior predictive check 2
plot BISoN edge weight estimates against point edge weight estimates (counts/observation time)

```{r}
rates_bison_lower <- exp(summary(fit_inla)$fixed[1:nr_dyads, 3])
rates_bison_upper <- exp(summary(fit_inla)$fixed[1:nr_dyads, 5])
rates_point_est <- df_obs_agg$count/df_obs_agg$total_samples

#check relationship with uncertainty
rates_uncertainty <- rates_bison_upper-rates_bison_lower 
plot(df_obs_agg$total_samples, rates_uncertainty)

y_min <- min(rates_bison_lower)
y_max <- max(rates_bison_upper)

plot(rates_point_est, rates_bison, col="#387780", ylim=c(y_min, y_max), xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(rates_point_est, rates_bison_lower, rates_point_est, rates_bison_upper, length=0)
abline(a=0, b=1)

# BISoN is slightly overestimating zeroes compared to point estimates (or the point estimates might be underestimating the edge weights) 
# similar to what mentioned above: this is because BISoN will never estimate tre zero's 
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
```

# extract edge weights
dyadic edge weights are now distributions which can be extracted as 
- a matrix with one column per dyad & one row per sample
- a distribution of adjacency matrices corresponding single draw of the posterior adjacency matrices 
  (stored in a 'tensor': a multidimensional matrix)
- an edgelist with uncertainty (i.e. 2.5% & 97.5% CI)

```{r}
######################################################
# matrix with one column per dyad & one row per sample
rates_samples <- exp(log_rate_samples) # exponentiating the log_p samples to get back at the real values
colnames(rates_samples) <- df_obs_agg$dyad_id

name_matrix <- paste0("matrices_edgeWeights/", groupyear, "_edge_weight_samples.csv")

write_csv(as.data.frame(rates_samples), name_matrix) 

######################################################
# distribution of adjacency matrices 
adj_tensor <- array(0, c(nr_ind, nr_ind, nr_samples)) # create empty multidimensional matrix
#ISSUE: the edgelist does not have entries for individuals with themselves

for (i in 1:nrow(df_obs_agg)) { # fill it with samples from the distribution
  adj_tensor[df_obs_agg[i,"ID2_id"][[1]], df_obs_agg[i,"ID1_id"][[1]],] <- rates_samples[1:nr_samples, i]
}
# View(adj_tensor[, , 1]) # see how 1 of adjacency those matrixes looks like

name_tensor <- paste0("tensor_AdjacencyMatrices/", groupyear, "_adjacency_matrices.rds")

saveRDS(adj_tensor, name_tensor)

######################################################
# edgelist with 2.5% & 97.5% CI
prob_edgelist <- round(exp(summary(fit_inla)$fixed[, c(1, 3, 5)]), 2)
colnames(prob_edgelist) <- c("50%", "2.5%", "97.5%")
prob_edgelist <- as.data.frame(prob_edgelist)
prob_edgelist <- cbind(dyad = substring(rownames(prob_edgelist),8), prob_edgelist)

name_edgelist <- paste0("edgelist_uncertainty/", groupyear, "_edgelist_with_uncertainty.csv")

write_csv(prob_edgelist, name_edgelist) 
```

# visualise uncertainty
to plot a network with a visualisation of the uncertainty around edge weights add a semi-transparent line around each edge with a width that corresponds to the uncertainty measure (the normalised difference between the 97.5% and 2.5% credible interval estimate for each edge weigh) to do this, generate two igraph objects (one for the main network & one for the uncertainty in edges) and plot them with the same coordinates, so they appear on top of each other

```{r, fig.width=8}
# calculate lower, median, and upper quantiles of edge weights
adj_quantiles <- apply(adj_tensor, c(1, 2), function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
adj_lower <- adj_quantiles[1, , ] # 2.5% credible interval
adj_mid <- adj_quantiles[2, , ]
adj_upper <- adj_quantiles[3, , ] # 97.5% credible interval
adj_range <- (adj_upper - adj_lower)

# set threshold for minimum edge weight of edges to be shown on network
threshold <- 0.15

# generate two igraph objects, one form the median and one from the error range
g_mid <- graph_from_adjacency_matrix(adj_mid * (adj_mid > threshold), mode="undirected", weighted=TRUE) # edges 
g_range <- graph_from_adjacency_matrix(adj_range * (adj_mid > threshold), mode="undirected", weighted=TRUE) # error around edges

# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3,vertex.label=NULL) 
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25), 
     vertex.label=unique_names, vertex.size=3,
     vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
```

# calculate network metrics
now that the edges are probabilities, incorporate that uncertainty further down into calculations of network metrics

```{r}

#####################
## NETWORK DENSITY ##

# calculating weighted density for the whole network 
dens <- rep(0, nr_samples) # create empty vector 
for (i in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
  s <- sum(adj_tensor[, , i])
  dens[i] <- s/nr_dyads 
}

# plot the distribution of the network density values
plot(density(dens), xlab="network density")

# save posterior draws of network density to use in further analyses
name_network_density <- paste0("Net_densities/", groupyear, "_posterior_network_density.csv")
write_csv(data.frame(draws=dens), name_network_density)

##########################
## Node weighted degree ##

w.degree <- matrix(0, nr_ind, nr_samples) # create empty vector 
row.names(w.degree)=unique_names;
for (s in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
  g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
  w.degree[,s] <- strength(g)
}


name_network_density <- paste0("Weighted_degree/", groupyear, "_weighted_degree.csv")
write_csv(data.frame(draws=w.degree), name_network_density)

```

# save workspace

```{r}
# save.image("~/Documents/MACAQUE R PACKAGE/BISoN/2. edgelists to BISoN.RData")
```
