p_visibility= p_behavior_visibility * p_terrain_visibility  # Final visibility, which is a combination of the sources of occlusions.
#Behavior studied
mean=60; sd=15
n_events = round(abs(rnorm(group_size, mean, sd))); #frequency of behavior
#number of behavioral events in the day per individual, or rarity of behavior
#Assuming each individual engages in the behavior at normally distributed frequencies)
#We could sample from another distribution (e.g. more skewed)
behavior_duration = 2 #behavior duration in sec
#Observation method
#NOTES:
# Not have a unique time per behavior? draw from a distribution around a mean?
# Visibility at individual, behavior and habitat level
#total observation time
n_hours = 7 #number of hours of observation in the day
time_in_s = n_hours*60*60 #number of seconds of observations in a day
#continuous focal
focal_duration_hrs = 1/12 #time of focal in hours (i.e. 10min)
focal_duration_s = focal_duration_hrs*3600 #focal duration in seconds
focal_break_time_min = 5 # minimum break time between focals in min
focal_break_time_s = focal_break_time_min*60 # minimum break time between focals in sec
n_focals = 20 # number of focals in a day
total_focal_h = n_focals*focal_duration_hrs #focal observation time in hours
total_focal_s = n_focals*focal_duration_s #focal observation time in seconds
#group scans
scan_obsTime_perID=1; #scan time needed per individual. Assuming 1sec
scan_duration = scan_obsTime_perID*group_size #duration of a scan observations in sec, scales with group size
scan_break_time_s = 60 # minimum break time between scans in seconds
num_scans = total_focal_s/scan_duration #equivalent # scans than the focal hours observed
#initiate outcomes of simulation
focaltime_perID = matrix(NA,n_simulations,group_size) #seconds of focal observation per ID
focalsamples_perID = matrix(NA,n_simulations,group_size)  #number of focal observations per ID
behav_timeObserved_focal_perID = matrix(NA,n_simulations,group_size)  #seconds of behavior observed per ID
behav_boutsObserved_focal_perID = matrix(NA,n_simulations,group_size)  # number of bouts observed per ID
focal_rate_perID = matrix(NA,n_simulations,group_size)  #observed rate of behavior per ID
behav_timeObserved_focal_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
#initiate outcomes of simulation
focaltime_perID = matrix(NA,n_simulations,group_size) #seconds of focal observation per ID
focalsamples_perID = matrix(NA,n_simulations,group_size)  #number of focal observations per ID
behav_timeObserved_focal_perID = matrix(NA,n_simulations,group_size)  #seconds of behavior observed per ID
behav_boutsObserved_focal_perID = matrix(NA,n_simulations,group_size)  # number of bouts observed per ID
focal_prop_perID = matrix(NA,n_simulations,group_size)  #obsevred proportion of time engaged in behavior X per ID
focal_rate_perID = matrix(NA,n_simulations,group_size)  #observed rate of behavior per ID
# behav_timeObserved_focal_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
# behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
if (any(is.nan(scan_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_rate_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_prop_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
y_max = max(c(max(plot_scan$y), max(plot_focal$y), max(plot_true$y)))
#Plot results
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_rate_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Rate per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true proportion per individual
geom_density(aes(true_prop_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_prop_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_prop_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_prop_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
y_max
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
summary(studentratings)
library(stringr)
library(igraph)
library(lubridate)
library(hms)
library(data.table)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(bisonR)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/")
source("Code/Functions/functions_GlobalNetworkMetrics.R")
group = c("F","V","KK","V","F","F","KK","V","V","KK","S","V","F","V")
years = c(2015,2015,2015,
2016,2016,2017,2017,2017,
2018, 2018,2019, 2019,2021,2021)
groupyears = c("F2015","V2015","KK2015",
"V2016","F2016","F2017",
"KK2017","V2017","V2018","KK2018",
"S2019","V2019","F2021","V2021")
gy=10#16
edgelist.all = data.frame()
savePath = '~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/'
#Load libraries
#bison package
library(bisonR)
library(dplyr)
#Survival analysis
library(survival)
library(survminer)
library(simsurv)
library(coxme)
#modelling
library(marginaleffects)
library(brms)
#network analysis
library(igraph)
#For plotting
library(ggplot2)
#multiple imputation
library(mice)
library(broom.mixed)
library(eha)
library(ehahelper)
#Set seed for reproducibility
set.seed(1234)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"proximity_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","KK","S","F")
years = c(2018, 2019, 2021, 2018, 2019, 2021)
groupyears = paste0(group,years)
#Initialize lists
gy=1; node_strength_all = list(); node_degree_all = list(); node_ids=list()
#Set parameters
num_iter = 20
priors <- get_default_priors("binary_conjugate")#define priors
priors$edge <- "beta(0.1,0.1)" # set less flat priors
#Load pre-hurricane network
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$weight = el$count/el$total_samples
el$year=factor(el$year)
#Fit post-hurricane network with bison
fit.el <- bison_model(
(count | total_samples) ~ dyad(ID1, ID2),
data=el,
model_type = "binary_conjugate", #Count conjugate for aggression data
priors=priors
)
#Draw n edgelists from the posterior of the network
samples.post<-draw_edgelist_samples(fit.el, num_draws=num_iter)
nodes = names(fit.el$node_to_idx)
#Draw network metrics from posterior (global, nodal and edge_weight)
node_degree <- extract_metric(fit.el, "node_degree[0.001]", num_draws =num_iter)
node_strength <- extract_metric(fit.el, "node_strength", num_draws =num_iter) #Output size: num_draws X num_nodes
num_iter
#Draw network metrics from posterior (global, nodal and edge_weight)
node_degree <- extract_metric(fit.el, "node_degree[0.001]", num_draws =num_iter)
#bison_proximity_survival.R
#Model proximity networks for Cayo data with bison and
#run downstream survival analyses
#C. Testard October 2022
#Load libraries
#bison package
library(bisonR)
library(dplyr)
#Survival analysis
library(survival)
library(survminer)
library(simsurv)
library(coxme)
#modelling
library(marginaleffects)
library(brms)
#network analysis
library(igraph)
#For plotting
library(ggplot2)
#multiple imputation
library(mice)
library(broom.mixed)
library(eha)
library(ehahelper)
#Set seed for reproducibility
set.seed(1234)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"grooming_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","KK","S","F")
years = c(2018, 2019, 2021, 2018, 2019, 2021)
groupyears = paste0(group,years)
#Initialize lists
gy=1; node_strength_all = list(); node_degree_all = list(); node_ids=list()
#Set parameters
num_iter = 20
priors <- get_default_priors("binary_conjugate")#define priors
priors$edge <- "beta(0.1,0.1)" # set less flat priors
#Load pre-hurricane network
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$weight = el$count/el$total_samples
el$year=factor(el$year)
el$weight = el$count/el$total_obseff
el$year=factor(el$year)
groupyears[gy]
groupyears[gy]=="V2018"|groupyears[gy]=="KK2018"
priors <- get_default_priors("binary_conjugate")#define priors
priors$edge <- "beta(0.1,0.1)" # set less flat priors
#prior_check(priors, "binary_conjugate")
#Fit post-hurricane network with bison
fit.el <- bison_model(
(count | total_obseff) ~ dyad(ID1, ID2),
data=el,
model_type = "binary_conjugate", #Count conjugate for aggression data
priors=priors
)
remotes::install_github("JHart96/bisonR@dev")
plot_predictions(fit.el, num_draws=20)
unqids = unique(c(el$ID1, el$ID2))
#Note: unique ID1 and ID2 nodes = n=1 (because self-edge do not exist)
ID1_nodes = el[match(unqids, el$ID1),c("ID1","ID1_obseff","ID1_sex","ID1_age",
"ID1_rank","group","year","isPost","groupyear")]
View(ID1_nodes)
ID2_nodes = el[match(unqids, el$ID2),c("ID2","ID2_obseff","ID2_sex","ID2_age",
"ID2_rank","group","year","isPost","groupyear")]
View(ID2_nodes)
nrow(ID1_nodes)
ID1_nodes[nrow(ID1_nodes),]=ID2_nodes[nrow(ID2_nodes),]
node.list[[gy]]<-ID1_nodes
el$ID1<-factor(el$ID1, levels=unqids)
el$ID2<-factor(el$ID2, levels=unqids)
priors <- get_default_priors("binary_conjugate")#define priors
priors$edge <- "beta(0.1,0.1)" # set less flat priors
#Fit post-hurricane network with bison
fit.el <- bison_model(
(count | total_obseff) ~ dyad(ID1, ID2),
data=el,
model_type = "binary_conjugate", #Count conjugate for aggression data
priors=priors
)
plot_predictions(fit.el, num_draws=20)
#Load pre-hurricane network
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$weight = el$count/el$total_obseff
el$year=factor(el$year)
unqids = unique(c(el$ID1, el$ID2))
#Note: unique ID1 and ID2 nodes = n=1 (because self-edge do not exist)
ID1_nodes = el[match(unqids, el$ID1),c("ID1","ID1_obseff","ID1_sex","ID1_age",
"ID1_rank","group","year","isPost","groupyear")]
ID2_nodes = el[match(unqids, el$ID2),c("ID2","ID2_obseff","ID2_sex","ID2_age",
"ID2_rank","group","year","isPost","groupyear")]
ID1_nodes[nrow(ID1_nodes),]=ID2_nodes[nrow(ID2_nodes),]
el$ID1<-factor(el$ID1, levels=unqids)
el$ID2<-factor(el$ID2, levels=unqids)
View(el)
