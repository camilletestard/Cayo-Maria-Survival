behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
#initiate outcomes of simulation
focaltime_perID = matrix(NA,n_simulations,group_size) #seconds of focal observation per ID
focalsamples_perID = matrix(NA,n_simulations,group_size)  #number of focal observations per ID
behav_timeObserved_focal_perID = matrix(NA,n_simulations,group_size)  #seconds of behavior observed per ID
behav_boutsObserved_focal_perID = matrix(NA,n_simulations,group_size)  # number of bouts observed per ID
focal_prop_perID = matrix(NA,n_simulations,group_size)  #obsevred proportion of time engaged in behavior X per ID
focal_rate_perID = matrix(NA,n_simulations,group_size)  #observed rate of behavior per ID
# behav_timeObserved_focal_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
# behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
if (any(is.nan(scan_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_rate_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_prop_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
y_max = max(c(max(plot_scan$y), max(plot_focal$y), max(plot_true$y)))
#Plot results
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_rate_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Rate per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true proportion per individual
geom_density(aes(true_prop_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_prop_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_prop_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_prop_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
y_max
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
summary(studentratings)
packages()
update.packages()
update.packages(ask=FALSE)
#Load libraries
#For plotting
library(ggplot2)
library(igraph)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"proximity_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","V","V","V","V",)#c("F","S","F","F","F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V")
years = c(2015,2016,2017,2018,2019,1021,2022)#c(2010,2011,2011,2012,2013,2013,2014,
# 2014,2015,2015,2015,2015,
# 2016,2016,2016,2016,2017,2017,2017,
# 2018,2018, 2019, 2019,2021,2021,2022,2022)
groupyears = paste0(group,years)
gy=1
for (gy in 1:length(groupyears)){ #for all group years
#Load network edgelist
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$year=factor(el$year)
el$weight = el$count/el$total_samples
weightedEL<-el[,c("ID1","ID2","weight")]
#Create adjacency matrix from edgelist
adjMat = dils::AdjacencyFromEdgelist(weightedEL)# create adjacency matrix based on edge list.
data = adjMat[["adjacency"]]; rownames(data) = adjMat[["nodelist"]]; colnames(data) = adjMat[["nodelist"]]
#read adjacency matrix into igraph
m=as.matrix(data) # coerces the data set as a matrix
am.g=graph.adjacency(m,mode= "undirected",weighted=T)
l <- layout.fruchterman.reingold(am.g, repulserad=vcount(am.g)^5,
area=vcount(am.g)^3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.2
V(am.g)$degree=igraph::degree(am.g)
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Results/SocialNetworks/Proximity/")
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
units="in", width=10, height=8, res=300, compression = 'lzw')
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
}
#Model proximity networks for Cayo data with bison and run downstream regression
#C. Testard October 2022
#Load libraries
#For plotting
library(ggplot2)
library(igraph)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"proximity_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","V","V","V","V")#c("F","S","F","F","F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V")
years = c(2015,2016,2017,2018,2019,2021,2022)#c(2010,2011,2011,2012,2013,2013,2014,
# 2014,2015,2015,2015,2015,
# 2016,2016,2016,2016,2017,2017,2017,
# 2018,2018, 2019, 2019,2021,2021,2022,2022)
groupyears = paste0(group,years)
gy=1
for (gy in 1:length(groupyears)){ #for all group years
#Load network edgelist
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$year=factor(el$year)
el$weight = el$count/el$total_samples
weightedEL<-el[,c("ID1","ID2","weight")]
#Create adjacency matrix from edgelist
adjMat = dils::AdjacencyFromEdgelist(weightedEL)# create adjacency matrix based on edge list.
data = adjMat[["adjacency"]]; rownames(data) = adjMat[["nodelist"]]; colnames(data) = adjMat[["nodelist"]]
#read adjacency matrix into igraph
m=as.matrix(data) # coerces the data set as a matrix
am.g=graph.adjacency(m,mode= "undirected",weighted=T)
l <- layout.fruchterman.reingold(am.g, repulserad=vcount(am.g)^5,
area=vcount(am.g)^3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.2
V(am.g)$degree=igraph::degree(am.g)
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Results/SocialNetworks/Proximity/")
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
units="in", width=10, height=8, res=300, compression = 'lzw')
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
}
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
units="in", width=10, height=8, res=300, compression = 'lzw')
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
width=10, height=8, res=300)
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
width=10, height=8)
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
#Model proximity networks for Cayo data with bison and run downstream regression
#C. Testard October 2022
#Load libraries
#For plotting
library(ggplot2)
library(igraph)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"proximity_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","V","V","V","V")#c("F","S","F","F","F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V")
years = c(2015,2016,2017,2018,2019,2021,2022)#c(2010,2011,2011,2012,2013,2013,2014,
# 2014,2015,2015,2015,2015,
# 2016,2016,2016,2016,2017,2017,2017,
# 2018,2018, 2019, 2019,2021,2021,2022,2022)
groupyears = paste0(group,years)
gy=1
for (gy in 1:length(groupyears)){ #for all group years
#Load network edgelist
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$year=factor(el$year)
el$weight = el$count/el$total_samples
weightedEL<-el[,c("ID1","ID2","weight")]
#Create adjacency matrix from edgelist
adjMat = dils::AdjacencyFromEdgelist(weightedEL)# create adjacency matrix based on edge list.
data = adjMat[["adjacency"]]; rownames(data) = adjMat[["nodelist"]]; colnames(data) = adjMat[["nodelist"]]
#read adjacency matrix into igraph
m=as.matrix(data) # coerces the data set as a matrix
am.g=graph.adjacency(m,mode= "undirected",weighted=T)
l <- layout.fruchterman.reingold(am.g, repulserad=vcount(am.g)^5,
area=vcount(am.g)^3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.2
V(am.g)$degree=igraph::degree(am.g)
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Results/SocialNetworks/Proximity/")
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
width=10, height=8)
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
}
#Model proximity networks for Cayo data with bison and run downstream regression
#C. Testard October 2022
#Load libraries
#For plotting
library(ggplot2)
library(igraph)
#Load data
data_path = "~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/"
load(paste0(data_path,"proximity_data.RData"))
edgelist.all$groupyear = paste0(edgelist.all$group, edgelist.all$year)
#Set group year list
group = c("V","V","V","V","S","V","V")#c("F","S","F","F","F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V")
years = c(2015,2016,2017,2018,2019,2021,2022)#c(2010,2011,2011,2012,2013,2013,2014,
# 2014,2015,2015,2015,2015,
# 2016,2016,2016,2016,2017,2017,2017,
# 2018,2018, 2019, 2019,2021,2021,2022,2022)
groupyears = paste0(group,years)
gy=5
#Load network edgelist
el = edgelist.all[edgelist.all$groupyear==groupyears[gy],]
el$year=factor(el$year)
el$weight = el$count/el$total_samples
weightedEL<-el[,c("ID1","ID2","weight")]
#Create adjacency matrix from edgelist
adjMat = dils::AdjacencyFromEdgelist(weightedEL)# create adjacency matrix based on edge list.
data = adjMat[["adjacency"]]; rownames(data) = adjMat[["nodelist"]]; colnames(data) = adjMat[["nodelist"]]
#read adjacency matrix into igraph
m=as.matrix(data) # coerces the data set as a matrix
am.g=graph.adjacency(m,mode= "undirected",weighted=T)
l <- layout.fruchterman.reingold(am.g, repulserad=vcount(am.g)^5,
area=vcount(am.g)^3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.2
V(am.g)$degree=igraph::degree(am.g)
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Results/SocialNetworks/Proximity/")
pdf(paste0("ProximityNetwork ",groupyears[gy],".pdf"),
width=10, height=8)
plot.igraph(am.g, layout=layout.sphere,
#vertex.label=V(am.g)$name, vertex.label.font=2,
vertex.size=10,
edge.color="grey20",
edge.width=E(am.g)$weight*20,edge.arrow.size = 0.5,edge.curved=0.5,
main = groupyears[gy])
dev.off()
