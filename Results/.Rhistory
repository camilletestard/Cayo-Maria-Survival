# behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
# behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
if (any(is.nan(scan_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_rate_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_prop_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
y_max = max(c(max(plot_scan$y), max(plot_focal$y), max(plot_true$y)))
#Plot results
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_rate_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Rate per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true proportion per individual
geom_density(aes(true_prop_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_prop_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_prop_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_prop_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
y_max
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
summary(studentratings)
packages()
update.packages()
update.packages(ask=FALSE)
library(ggplot2)
library(furrr)
source("Simulation Scripts/simulation_functions.r")
#source("~/Documents/Github/rethinking-obs-methods/Simulation Scripts/simulation_functions.r")
set.seed(1234)
source("~/Documents/Github/rethinking-obs-methods/Simulation Scripts/simulation_functions.r")
seq(30, 180, by = 5)
simulation_parameters <-
list(
n_days = seq(30, 180, by = 5),
# number of observation days to simulate (assume 7h per day, see below)
group_size = seq(10, 100, by = 5),
p_terrain_visibility = seq(0.1, 1, by = 0.1),
p_behavior_visibility = seq(0.1, 1, by = 0.1),
mean_events = c(seq(1, 19, by = 1),
seq(20, 50, by = 5)),
# mean number of behavioral events per day, per individual (sd set as mean/3, see below)
behavior_duration = c(seq(1, 9, by = 1),
seq(10, 55, by = 5),
seq(60, 110, by = 10),
seq(120, 600, by = 60)),
# behavior duration in sec
focal_duration_min = c(seq(5, 25, by = 5),
seq(30, 60, by = 10)),
# time of focal observation in minutes
focal_break_time_min = 5,
# minimum break time between focals in minutes
scan_obsTime_perID = seq(1, 11, by = 2),
# scan time needed per individual in seconds
scan_break_time_min = c(seq(5, 25, by = 5),
seq(30, 60, by = 10))
) # minimum break time between end of scan and start of new one in seconds
i = 1
lapply(simulation_parameters, sample, 1)
n_events <-
round(# if not given, calculate average number of daily interactions per individual for this simulation run
abs(
rnorm(
# assuming normal distribution of events per individual
sim_values$group_size[i],
# for each individual in the group pick a number of daily interactions from a normal distribution with
sim_values$mean_events[i],
# mean_events as set in simulations_parameters and
2
)
)) + 1 # sd set as mean_events/3, a reasonable variation that will keep a similar distribution for different means
# add 1 because sometimes there would be 0s and then it cracks
print(paste(c(sim_values, sample(1:1000, 1)), collapse = '_'))
i = 1
sim_values <- lapply(simulation_parameters, sample, 1)
n_events <-
round(# if not given, calculate average number of daily interactions per individual for this simulation run
abs(
rnorm(
# assuming normal distribution of events per individual
sim_values$group_size[i],
# for each individual in the group pick a number of daily interactions from a normal distribution with
sim_values$mean_events[i],
# mean_events as set in simulations_parameters and
2
)
)) + 1 # sd set as mean_events/3, a reasonable variation that will keep a similar distribution for different means
# add 1 because sometimes there would be 0s and then it cracks
print(paste(c(sim_values, sample(1:1000, 1)), collapse = '_'))
behavior_duration = 10
n_hours = 7*90
time_in_s <-
n_hours * 60 * 60 # number of seconds of observations in a study period
beh_time_seq <- seq.int(behavior_duration, floor(time_in_s - behavior_duration),
by = behavior_duration)
load("ProxMdlOutput_PrePost.RData")#Longitudinal change in proximity
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/") #Set working directory
load("ProxMdlOutput_PrePost.RData")#Longitudinal change in proximity
load("AggMdlOutput_PrePost.RData")#Longitudinal change in aggression
tab_model(mdl.strength.proxPrePost.perYear)
library(mice)
library(broom.mixed)
library(eha)
library(ehahelper)
library(ggplot2)
library(dplyr)
library(sjPlot)
tab_model(mdl.strength.proxPrePost.perYear)
summary(mice::pool(mdl.strength.aggPrePost.perYear ))
summary(mice::pool(mdl.strength.proxPrePost.perYear ))
library(ggplot2)
library(lme4)
library(dplyr)
library(forcats)
library(ggbeeswarm)
library(glmmTMB)
#Load data
setwd('~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/')
load("SocialCapital.ALL.RData")
table(SocialCapital.ALL$id, SocialCapital.ALL$year)
t<-table(SocialCapital.ALL$id, SocialCapital.ALL$year)
colSums(t)
rowSums(t)
hist(rowSums(t))
mean(rowSums(t))
sd(rowSums(t))
table(SocialCapital.ALL$id, SocialCapital.ALL$isPost)
rowSums(t)
length(which(rowSums(t)==1))
nrow(t2)
t2<-table(SocialCapital.ALL$id, SocialCapital.ALL$isPost)
length(which(rowSums(t2)==1))
nrow(t2)
(790-241)/790
(790-241)
load("Survival_Adults_allGroups.RData")
unique(SurvivalData$id)
length(unique(SurvivalData$id))
hist(rowSums(t), bins=30)
hist(rowSums(t), nbins=30)
hist(rowSums(t), breaks=30)
hist(rowSums(t), breaks=20)
hist(rowSums(t), breaks=10)
table(SurvivalData$id, SurvivalData$Status)
t3<-table(SurvivalData$id, SurvivalData$Status)
which(t3[,1]!=0)
length(which(t3[,1]!=0))
table(SurvivalData$id, SurvivalData$YearOfDeath)
load("Survival_Adults.RData")
#Generate input data for survival models.
#Feburary 2022, Camille Testard
library(stringr)
library(igraph)
library(lubridate)
library(data.table)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
#Load scan data and population info
setwd("~/Dropbox (Penn)/CayoBehavior/Data/Census")
census = xlsx::read.xlsx("2021-12-16-CENSO-FINAL.xlsx", 1)
#Update census to consider deaths in 2022
death_update2022 =xlsx::read.xlsx("Deaths2022.xlsx",1)
death_update2022$Status = "DEAD"
id=1
for (id in 1:nrow(death_update2022)){
idx=which(census$AnimalID == death_update2022$AnimalID[id])
if(!purrr::is_empty(idx)){
census[idx,c("DOD","Status")]=death_update2022[id, c("DOD","Status")]
}
}
#pedigree = read.csv("/PEDIGREE_2021.txt", sep = '\t')
rm(list = setdiff(ls(), "census"))
load("Survival_Adults_TimeVarying_allgroups.RData")
#Load data
setwd('~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/')
load("Survival_Adults_TimeVarying_allgroups.RData")
length(unique(SurvivalData.ALL$id))
table(SurvivalData$id, SurvivalData$YearOfDeath)
table(SurvivalData.ALL$id, SurvivalData.ALL$YearOfDeath)
t3<-table(SurvivalData.ALL$id, SurvivalData.ALL$YearOfDeath)
t3[t3>1]=1
t3
rowSums(t3)
colSums(t3)
sum(colSums(t3))
Death_Per_Year = colSums(t3)
sum(Death_Per_Year[1:5])
sum(Death_Per_Year[6:10])
t<-table(SocialCapital.ALL$id, SocialCapital.ALL$year)
load("SocialCapital.ALL.RData")
t2<-table(SocialCapital.ALL$id, SocialCapital.ALL$isPost)
length(which(rowSums(t2)==1))/nrow(t2)
t2
t2[t2>1]=1
t2
pre.only = length(which(t2[,2]==1 & t2[,1]==0))
pre.only
post.only = length(which(t2[,2]==0 & t2[,1]==1))
post.only
pre.and.post = length(which(t2[,2]==1 & t2[,1]==1))
pre.and.post
pre = length(which(t2[,2]==1)
pre
length(which(t2[,2]==1))
post = length(which(t2[,1]==1))
post
length(which(t2[,2]==1)) #pre-hurricane
length(which(t2[,1]==1)) #post-hurricane
length(which(t2[,2]==1 & t2[,1]==0)) #pre-hurricane only
length(which(t2[,1]==1)) #post-hurricane
sum(Death_Per_Year[6:10])#post-hurricane
View(SurvivalData.ALL)
install.packages("VennDiagram")
set1 <- paste(rep("word_" , 200) , sample(c(1:1000) , 200 , replace=F) , sep="")
set2 <- paste(rep("word_" , 200) , sample(c(1:1000) , 200 , replace=F) , sep="")
set3 <- paste(rep("word_" , 200) , sample(c(1:1000) , 200 , replace=F) , sep="")
which(t2[,2]==1)
names(which(t2[,2]==1))
#Ven diagram of IDs pre
set1 = names(which(t2[,2]==1))
set2 = names(which(t2[,1]==1))
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"))
library("VennDiagram")
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"))
#Ven diagram of IDs pre
setwd('~/Documents/GitHub/Cayo-Maria-Survival/Results/')
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"),
filename = 'venn_diagramm.pdf',
output=TRUE)
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"),
filename = 'venn_diagramm.pdf',
output=TRUE,
# Output features
imagetype="pdf" ,
height = 480 ,
width = 480 ,
resolution = 300,
compression = "lzw",
# Circles
lwd = 2,
lty = 'blank',
fill = myCol,
# Numbers
cex = .6,
fontface = "bold",
fontfamily = "sans")
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"),
filename = 'venn_diagramm.pdf',
output=TRUE,
# Output features
imagetype="pdf" ,
height = 480 ,
width = 480 ,
resolution = 300,
compression = "lzw",
# Circles
lwd = 2,
lty = 'blank',
fill = "red",
# Numbers
cex = .6,
fontface = "bold",
fontfamily = "sans")
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"),
filename = 'venn_diagramm.pdf',
output=TRUE,
# Output features
imagetype="eps" ,
height = 480 ,
width = 480 ,
resolution = 300,
compression = "lzw",
# Circles
lwd = 2,
lty = 'blank',
fill = "red",
# Numbers
cex = .6,
fontface = "bold",
fontfamily = "sans")
venn.diagram(
x = list(set1, set2),
category.names = c("pre" , "post"),
filename = 'venn_diagramm.pdf',
output=TRUE,
# Output features
imagetype="png" ,
height = 480 ,
width = 480 ,
resolution = 300,
compression = "lzw",
# Circles
lwd = 2,
lty = 'blank',
fill = "red",
# Numbers
cex = .6,
fontface = "bold",
fontfamily = "sans")
length(which(t2[,2]==1)) #pre-hurricane
length(which(t2[,1]==1)) #post-hurricane
length(which(t2[,2]==1 & t2[,1]==0)) #pre-hurricane only
length(which(t2[,2]==0 & t2[,1]==1))#post-hurricane only
#Get the number of years each individual is sampled
t<-table(SocialCapital.ALL$id, SocialCapital.ALL$year)
t
t[t>1]=1
#Get the number of years each individual is sampled
t<-table(SocialCapital.ALL$id, SocialCapital.ALL$year)
rowSums(t)
t
#Get the number of years each individual is sampled
t<-table(SocialCapital.ALL$id, SocialCapital.ALL$year)
t[t>1]=1
t
rowSums(t)
mean(rowSums(t))
sd(rowSums(t))
numYears_perID = rowSums(t)
length(which(numYears_perID==1))
histogram(rowSums(t), breaks=10)
hist(rowSums(t), breaks=10)
hist(numYears_perID, breaks=10)
hist(numYears_perID, breaks=15)
length(which(numYears_perID==2))
length(which(numYears_perID==3))
length(which(numYears_perID==4))
length(which(numYears_perID==5))
density(numYears_perID)
dens(numYears_perID)
plot(density(numYears_perID))
