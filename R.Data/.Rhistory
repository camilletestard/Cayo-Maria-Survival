# plot the median graph first and then the error range to show uncertainty over edges # NEEDS TWEAKING TO MAKE FIGURE LOOK GOOD LATER
coords <- igraph::layout_nicely(g_mid)
# coords <- igraph::layout.circle(g_mid)
plot(g_mid, edge.width=2 * E(g_mid)$weight, edge.color="black",  layout=coords, vertex.size=3,vertex.label=NULL)
plot(g_mid, edge.width=5 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25),
vertex.label=unique_names, vertex.size=3,
vertex.label.dist=0.5, vertex.label.color="black", layout=coords, add=TRUE)
dens <- rep(0, nr_samples) # create empty vector
for (i in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
s <- sum(adj_tensor[, , i])
dens[i] <- s/nr_dyads
}
# plot the distribution of the network density values
plot(density(dens), xlab="network density")
# save posterior draws of network density to use in further analyses
name_network_density <- paste0("Net_densities/", groupyear, "_posterior_network_density.csv")
write_csv(data.frame(draws=dens), name_network_density)
w.degree <- matrix(0, nr_ind, nr_samples) # create empty vector
row.names(w.degree)=as.character(unique(df_obs_agg$ID1));
w.degree <- matrix(0, nr_ind, nr_samples) # create empty vector
row.names(w.degree)=unique_names;
for (s in 1:nr_samples) { # and fill with densities calculated from samples taken from edge posterior probabilities
g = graph.adjacency(adj_tensor[, , s],mode= "undirected",weighted=T)
w.degree[,s] <- strength(g)
}
w.degree[,s]
log_rate_mu <- 0
log_rate_sigma <- 1.75
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
# empty global environment
rm(list=ls())
# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Bayesian_code_bison/")
#source("scripts/simulations.R")
source("scripts/sampler.R")
source("scripts/functions_GlobalNetworkMetrics.R")
group= "V"
year=2016
groupyear = paste(group,year,sep="")
#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))
if (groupyear == "V2019"){ #quick fix for now
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)]=
paste(prox_data$focal.monkey[!is.na(prox_data$partners.activity..sequential.)],
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)],sep=",")
}
#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id#[meta_data$focalcutoff_met=="Y"]
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")
#extract the number of unique IDs
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA
#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))
df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)]
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples
## Add id qualifiers
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"
head(df_obs_agg)
save(df_obs_agg,file = "Proximity_df.RData")
#NOTES:
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.
log_rate_mu <- 0
log_rate_sigma <- 1.75
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
View(df_obs_agg)
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
nr_samples <- 1000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix
inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution
for (i in 1:nr_samples) { # fill in matrix
log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
# sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model
#cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison) #Used to check the ordering of dyads
jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)
plot(
df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd),
rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) + rnorm(nr_dyads, sd=jitter_sd),
col=rgb(0, 0, 1, jitter_sd),
xlab="Observed event counts",
ylab="Predicted event counts"
)
for (i in 1:20) {
points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) +
rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)
# the model never will model true zero's because it can never be certain a dyad never interacts (absence of evidence)
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
rates_bison_lower <- exp(summary(fit_inla)$fixed[1:nr_dyads, 3])
rates_bison_upper <- exp(summary(fit_inla)$fixed[1:nr_dyads, 5])
rates_point_est <- df_obs_agg$count/df_obs_agg$total_samples
#check relationship with uncertainty
rates_uncertainty <- rates_bison_upper-rates_bison_lower
plot(df_obs_agg$total_samples, rates_uncertainty)
y_min <- min(rates_bison_lower)
y_max <- max(rates_bison_upper)
plot(rates_point_est, rates_bison, col="#387780", ylim=c(y_min, y_max), xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(rates_point_est, rates_bison_lower, rates_point_est, rates_bison_upper, length=0)
abline(a=0, b=1)
# BISoN is slightly overestimating zeroes compared to point estimates (or the point estimates might be underestimating the edge weights)
# similar to what mentioned above: this is because BISoN will never estimate tre zero's
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
log_rate_mu <- 0
log_rate_sigma <- 1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
log_rate_mu <- 0
log_rate_sigma <- 1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
log_rate_mu <- 0
log_rate_sigma <- 1.75
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
View(log_rate_samples)
# empty global environment
rm(list=ls())
# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Bayesian_code_bison/")
#source("scripts/simulations.R")
source("scripts/sampler.R")
source("scripts/functions_GlobalNetworkMetrics.R")
group= "V"
year=2016
groupyear = paste(group,year,sep="")
#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))
if (groupyear == "V2019"){ #quick fix for now
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)]=
paste(prox_data$focal.monkey[!is.na(prox_data$partners.activity..sequential.)],
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)],sep=",")
}
#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id#[meta_data$focalcutoff_met=="Y"]
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")
#extract the number of unique IDs
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA
#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))
df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)]
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples
## Add id qualifiers
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"
head(df_obs_agg)
save(df_obs_agg,file = "Proximity_df.RData")
#NOTES:
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.
log_rate_mu <- 0
log_rate_sigma <- 1.75
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
log_rate_mu <- 0
log_rate_sigma <- 1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
log_rate_mu <- 0
log_rate_sigma <- 1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
log_rate_mu <- 0
log_rate_sigma <-2
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_obs_time)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
# empty global environment
rm(list=ls())
# load packages
library(rstan)
library(INLA)
library(tidyverse)
library(dplyr)
library(magrittr)
library(igraph)
#Load functions
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Bayesian_code_bison/")
#source("scripts/simulations.R")
source("scripts/sampler.R")
source("scripts/functions_GlobalNetworkMetrics.R")
group= "V"
year=2016
groupyear = paste(group,year,sep="")
#Load cayo data
dataPath = '~/Documents/GitHub/Cayo-Maria-Survival/Data All Cleaned/BehavioralDataFiles/'
prox_data = read.csv(paste(dataPath,"Group",groupyear,"_ProximityGroups.txt", sep=""))
meta_data = read.csv(paste(dataPath,"Group",groupyear,"_GroupByYear.txt", sep=""))
if (groupyear == "V2019"){ #quick fix for now
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)]=
paste(prox_data$focal.monkey[!is.na(prox_data$partners.activity..sequential.)],
prox_data$in.proximity[!is.na(prox_data$partners.activity..sequential.)],sep=",")
}
#Format data with aggregate format
# Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = meta_data$id#[meta_data$focalcutoff_met=="Y"]
edgelist = calcMasterEL(unqIDs);
df_obs_agg  = calcEdgeList(prox_data, edgelist);
names(df_obs_agg)=c("ID1", "ID2", "dyad_id","count")
#extract the number of unique IDs
unique_names <- unique(c(df_obs_agg$ID1, df_obs_agg$ID2))
nr_ind <- length(unique_names)
nr_dyads <- nr_ind*(nr_ind-1)/2 # -1 to remove self-interactions e.g. AA & /2 because undirected so AB = BA
#Get observation effort for each dyad
numscans = as.data.frame(table(prox_data$focal.monkey))
df_obs_agg$ID1_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID1_obseff_samples = numscans$Freq[match(df_obs_agg$ID1, numscans$Var1)]
df_obs_agg$ID2_obseff_duration = meta_data$hrs.focalfollowed[match(df_obs_agg$ID2, meta_data$id)]
df_obs_agg$ID2_obseff_samples = numscans$Freq[match(df_obs_agg$ID2, numscans$Var1)]
df_obs_agg$total_obs_time = df_obs_agg$ID1_obseff_duration  + df_obs_agg$ID2_obseff_duration
df_obs_agg$total_samples = df_obs_agg$ID1_obseff_samples + df_obs_agg$ID2_obseff_samples
## Add id qualifiers
df_obs_agg$ID1 = factor(df_obs_agg$ID1, levels = unique_names); df_obs_agg$ID2 = factor(df_obs_agg$ID2, levels = unique_names);
df_obs_agg$ID1_id = as.integer(df_obs_agg$ID1); df_obs_agg$ID2_id = as.integer(df_obs_agg$ID2)
df_obs_agg$dyad_id = factor(df_obs_agg$dyad_id, levels=df_obs_agg$dyad_id)
#sex
df_obs_agg$ID1_sex = meta_data$sex[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_sex = meta_data$sex[match(df_obs_agg$ID2, meta_data$id)]
#rank
df_obs_agg$ID1_rank = meta_data$ordinal.rank[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_rank = meta_data$ordinal.rank[match(df_obs_agg$ID2, meta_data$id)]
#age
df_obs_agg$ID1_age = meta_data$age[match(df_obs_agg$ID1, meta_data$id)]
df_obs_agg$ID2_age = meta_data$age[match(df_obs_agg$ID2, meta_data$id)]
#group, year, Hurricane status
df_obs_agg$group = group; df_obs_agg$year = year; df_obs_agg$isPost = "post"
head(df_obs_agg)
save(df_obs_agg,file = "Proximity_df.RData")
#NOTES:
#1. Aggregated dataframe does not have observation name. Cannot account for non-independence of datapoints or other observation-level effects.
log_rate_mu <- 0
log_rate_sigma <-2
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
log_rate_mu <- 0
log_rate_sigma <-1.5
log_rates <- rnorm(1e5, log_rate_mu, log_rate_sigma) # sampling from the priors for visualisation
plot(density(exp(log_rates))) # the peak at 0 is misleading, note the very small densities: it's only a tiny peak actually
prior.fixed <- list(mean=log_rate_mu, prec=1/log_rate_sigma) # specify the priors
# note that in INLA it uses precision instead of SD (which is 1/SD)
#fit the bayesian model. Get posterior distribution from priors + data input
fit_inla <- inla(
count ~ 0 + dyad_id + offset(log(total_samples)), # model edge weights with offset term for observation effort
family="poisson", # poisson for frequency/count data
data=df_obs_agg,
control.fixed=prior.fixed,
control.compute=list(config = TRUE)
)
nr_samples <- 1000 #number of samples to extract from the posterior distribution
log_rate_samples <- matrix(0, nr_samples, nr_dyads) # create empty matrix
inla_samples <- inla.posterior.sample(nr_samples, fit_inla)#get samples from posterior distribution
for (i in 1:nr_samples) { # fill in matrix
log_rate_samples[i, ] <- tail(inla_samples[[i]]$latent, nr_dyads)
# sampling happens across all dyads simultaneously (which is important if dyads are non-independent, e.g. with group scans)
}
rates_bison <- exp(summary(fit_inla)$fixed[1:nr_dyads, 1]) #extract rates from the fitted model
#cbind(as.character(df_obs_agg$dyad_id), df_obs_agg$count, rates_bison) #Used to check the ordering of dyads
jitter_sd <- 0.25 # set jitter lower to see real values (but then more overlap)
plot(
df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd),
rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) + rnorm(nr_dyads, sd=jitter_sd),
col=rgb(0, 0, 1, jitter_sd),
xlab="Observed event counts",
ylab="Predicted event counts"
)
for (i in 1:20) {
points(df_obs_agg$count + rnorm(nr_dyads, sd=jitter_sd), rpois(nr_dyads, rates_bison * df_obs_agg$total_samples) +
rnorm(nr_dyads, sd=jitter_sd), col=rgb(0, 0, 1, 0.25))
}
abline(a=0, b=1)
# the model never will model true zero's because it can never be certain a dyad never interacts (absence of evidence)
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
rates_bison_lower <- exp(summary(fit_inla)$fixed[1:nr_dyads, 3])
rates_bison_upper <- exp(summary(fit_inla)$fixed[1:nr_dyads, 5])
rates_point_est <- df_obs_agg$count/df_obs_agg$total_samples
#check relationship with uncertainty
rates_uncertainty <- rates_bison_upper-rates_bison_lower
plot(df_obs_agg$total_samples, rates_uncertainty)
y_min <- min(rates_bison_lower)
y_max <- max(rates_bison_upper)
plot(rates_point_est, rates_bison, col="#387780", ylim=c(y_min, y_max), xlab="Point estimates of edge weights", ylab="BISoN estimates of edge weights")
arrows(rates_point_est, rates_bison_lower, rates_point_est, rates_bison_upper, length=0)
abline(a=0, b=1)
# BISoN is slightly overestimating zeroes compared to point estimates (or the point estimates might be underestimating the edge weights)
# similar to what mentioned above: this is because BISoN will never estimate tre zero's
# if there are (many) true zero's expected in the data, it might be good to add a zero-inflated part that can model true zero's as edges
# this will depend on the behaviour observed (e.g. more 0's in grooming than proximity) & other factors (e.g. more 0's in male interactions)
library(rstan)
library(igraph)
library(tidyverse)
data <- readRDS("../example_data/binary.RData")
df <- data$df
df_agg <- data$df_agg
logit_edge_samples <- data$logit_edge_samples
View(df_agg)
View(logit_edge_samples)
dim(logit_edge_samples)
library(rstan)
library(igraph)
library(tidyverse)
data <- readRDS("../example_data/binary.RData")
df <- data$df
df_agg <- data$df_agg
logit_edge_samples <- data$logit_edge_samples
model_nodal <- stan_model("../models/nodal_regression.stan")
View(logit_edge_samples)
edge_samples <- plogis(logit_edge_samples)
View(edge_samples)
library(ggplot2)
library(forcats)
library(lme4)
#Load data
setwd('~/Documents/GitHub/Cayo-Maria-Survival/R.Data/')
load("SocialCapital.ALL.RData")
###################
### Format data ###
###################
SocialCapital.ALL<-within(SocialCapital.ALL,{
sex<-as.factor(sex)
ordinal.rank<-as.factor(ordinal.rank)
id<-as.factor(id)
isPost<-as.factor(isPost)
isPost.year<-as.factor(isPost.year)
year <- as.factor(year)
})
SocialCapital.ALL$group.year = paste(SocialCapital.ALL$group, SocialCapital.ALL$year, sep="")
#If only consider IDs present both pre and post
id.isPost = table(SocialCapital.ALL$id, SocialCapital.ALL$isPost)
id_pre = row.names(as.data.frame(which(id.isPost[,"pre"]>0))); id_post = row.names(as.data.frame(which(id.isPost[,"post"]>0)))
id.PreAndPost = as.data.frame(row.names(as.data.frame(which(id.isPost[,"pre"]>0 & id.isPost[,"post"]>0)))); names(id.PreAndPost)="id"
id.PreAndPost$group = SocialCapital.ALL$group[match(id.PreAndPost$id, SocialCapital.ALL$id)]
table(id.PreAndPost$group)
SocialCapital.prepost = subset(SocialCapital.ALL, id %in% id.PreAndPost$id)
data = SocialCapital.prepost; data = subset(data, group %in% c("KK","F","V")) #Only consider within-individual comparisons
#data = SocialCapital.ALL #OR whole data
only.post<- droplevels(subset(data, isPost.year %in% c("post2018","post2019","post2021")))
data %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
ggplot(aes(x=year, y=agg.rate, fill=isPost))+
geom_violin(scale ="width")+ geom_jitter(width = 0.1, alpha = 0.7)+
theme_light(base_size=15)+theme(axis.text.x = element_text(angle = 45))+
ylab('Aggression rate')+ xlab('Year')+
geom_vline(xintercept = 3.5, linetype = "dashed", colour = "red")
data %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
ggplot(aes(x=year, y=agg.rate, fill=isPost))+
geom_violin(scale ="width")+ geom_jitter(width = 0.1, alpha = 0.7)+
theme_light(base_size=15)+theme(axis.text.x = element_text(angle = 45))+
ylab('Aggression rate')+ xlab('Year')+ labs(fill="xyz")+
geom_vline(xintercept = 3.5, linetype = "dashed", colour = "red")
data %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
ggplot(aes(x=year, y=agg.rate, fill=isPost))+
geom_violin(scale ="width")+ geom_jitter(width = 0.1, alpha = 0.7)+
theme_light(base_size=15)+theme(axis.text.x = element_text(angle = 45))+
ylab('Aggression rate')+ xlab('Year')+ labs(fill="Hurricane Status")
data %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
ggplot(aes(x=year, y=prob.prox, fill=isPost))+
geom_violin(scale ="width")+ geom_jitter(width = 0.1, alpha = 0.7)+
geom_vline(xintercept = 3.5, linetype=2)+
theme_light(base_size=15)+theme(axis.text.x = element_text(angle = 45))+
ylab('P(proximity)')+ xlab('Year')+ labs(fill="Hurricane Status")+
geom_vline(xintercept = 3.5, linetype = "dashed", colour = "red")
