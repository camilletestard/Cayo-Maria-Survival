for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
#initiate outcomes of simulation
focaltime_perID = matrix(NA,n_simulations,group_size) #seconds of focal observation per ID
focalsamples_perID = matrix(NA,n_simulations,group_size)  #number of focal observations per ID
behav_timeObserved_focal_perID = matrix(NA,n_simulations,group_size)  #seconds of behavior observed per ID
behav_boutsObserved_focal_perID = matrix(NA,n_simulations,group_size)  # number of bouts observed per ID
focal_prop_perID = matrix(NA,n_simulations,group_size)  #obsevred proportion of time engaged in behavior X per ID
focal_rate_perID = matrix(NA,n_simulations,group_size)  #observed rate of behavior per ID
# behav_timeObserved_focal_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
scansamples_perID = matrix(NA,n_simulations,group_size) # number of scan samples per ID
behav_boutsObserved_scan_perID = matrix(NA,n_simulations,group_size) # number of bouts observed per ID
scan_rate_perID = matrix(NA,n_simulations,group_size) #observed rate of behavior per ID
# behav_timeObserved_scan_total = matrix(NA,1,n_simulations) #seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total = matrix(NA,1,n_simulations) #number of bouts observed for all IDs
true_prop_behav_perID = matrix(NA,n_simulations,group_size)
true_rate_behav_perID = matrix(NA,n_simulations,group_size)
sim=1
for (sim in 1:n_simulations){ #for all simulation simations
#Initialize matrices
# size: [number IDs X Time in sec]
behavior = matrix(0, group_size, time_in_s) #Real behaviors
focals = matrix(0, group_size, time_in_s) #Focal observation time points
scans = matrix(0, group_size, time_in_s) #Group scans time points
#######################
#Create behavior matrix
#randomly assign mid-point of behavior during the day for each individual
id=1; e=1; event_id=1
for (id in 1:group_size){ #For each individual
event_times = sample(seq(behavior_duration, time_in_s-behavior_duration,
by = behavior_duration), n_events[id] )#sample behavioral events times (mid-point of behavior)
#Sample behavioral event times during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently min time lapse = length of the behavior
#Fill in the 'real' behavior matrix
for (e in 1:length(event_times)){ #for all behavior events
behavior[id,(event_times[e]-behavior_duration/2+1):(event_times[e]+behavior_duration/2)]=event_id
#Set each behavior bout in the day with a unique identifier event_id
event_id=event_id+1
}
}
########################################################
#Find observed behaviors using continuous focal sampling
#Assign focal times during the day
#Set focal list for the day
if (group_size<n_focals){ #if there are less individuals than #focals in a day
focal_id_list=sample(1:group_size, n_focals, replace = T)
}else{ #If there are more or equal #individuals in a day
focal_id_list=sample(1:group_size, n_focals)}
#Set focal mid-points
focal_times = sample(seq(focal_duration_s,time_in_s-focal_duration_s,
by=focal_break_time_s), n_focals)
#Sample focal mid time points during the day with a minimum
#time lapse between behavioral events ('by' time).
#Currently time lapse = focal_break_time
#Fill in the 'focal' behavior matrix
for (f in 1:n_focals){ #For each focal
focals[focal_id_list[f],
(focal_times[f]-focal_duration_s/2+1):(focal_times[f]+focal_duration_s/2)]=1
}
#Data observed during focals
observed_behavior_focal = behavior*focals
##############################################
#Find observed behaviors using group scans
#Assign scan times during the day
if (length(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s))>num_scans){
#if potential scan mid-points with regular break time in sec > number of scans for the day
# Randomly set scan mid-points
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_break_time_s), num_scans)
}else{ # else, change break time to scan duration
scan_times = sample(seq(scan_duration,time_in_s-scan_duration,by=scan_duration), num_scans)}
sc=1
for (sc in 1:num_scans){ #For each scan
ids = sample(1:group_size, round(p_visibility*group_size)) #select individuals that are visible
scan_epoch = (scan_times[sc]-scan_duration/2+1):(scan_times[sc]+scan_duration/2) #scan time points
#Stagger visibility of individuals (one ID per second)
epoch = sample(1:length(scan_epoch), length(scan_epoch))
id=1
for (id in 1:length(ids)){ #for each visible id
scans[ids[id],scan_epoch[epoch[id]]]=1 #randomly scan observation to each visible ID (1 per sec)
}
}
#Data observed during scans
observed_behavior_scan = behavior*scans
##############################################
#Evaluate scan vs. continuous sampling -based behavior observed
#True rates/proportion
true_prop_behav_perID[sim,] = n_events*behavior_duration/time_in_s #true proportion of time engaged in behavior X per ID
true_rate_behav_perID[sim,] = n_events/time_in_s #true rate of behavior X per ID
#Continuous-sampling-based estimates
focaltime_perID[sim,] = rowSums(focals!=0) #seconds of continuous observation per ID
focalsamples_perID[sim,] = rowSums(focals!=0)/focal_duration_s #number of focal observations per ID
behav_timeObserved_focal_perID[sim,] = rowSums(observed_behavior_focal!=0) #seconds of behavior observed per ID
behav_boutsObserved_focal_perID[sim,] = apply(observed_behavior_focal,1,function(x) length(unique(x))-1) # number of bouts observed per ID
focal_prop_perID[sim,] = behav_timeObserved_focal_perID[sim,]/rowSums(focals!=0) #observed proportion of time of behavior per ID
focal_rate_perID[sim,] = behav_boutsObserved_focal_perID[sim,]/rowSums(focals!=0) #observed rate of behavior per ID
# behav_timeObserved_focal_total[sim] = length(which(observed_behavior_focal!=0)) #seconds of behavior observed for all IDs
# behav_boutsObserved_focal_total[sim] = length(unique(observed_behavior_focal[which(observed_behavior_focal!=0)])) #number of bouts observed for all IDs
#Scan-sampling-based estimates
scansamples_perID[sim,] = rowSums(scans!=0) # number of scan samples per ID
behav_boutsObserved_scan_perID[sim,] = rowSums(observed_behavior_scan!=0) # number of bouts observed per ID
scan_rate_perID[sim,] = rowSums(observed_behavior_scan!=0)/rowSums(scans!=0) #observed probability of occurrence of behavior per ID
# behav_timeObserved_scan_total[sim] = length(which(observed_behavior_scan!=0))#seconds of behavior observed for all IDs
# behav_boutsObserved_scan_total[sim] = length(unique(observed_behavior_scan[which(observed_behavior_scan!=0)])) #number of bouts observed for all IDs
print(sim)
}
#Remove NAs
if (any(is.nan(focal_prop_perID))){ focal_prop_perID[is.nan(focal_prop_perID)]=0 }
if (any(is.nan(focal_rate_perID))){ focal_rate_perID[is.nan(focal_rate_perID)]=0 }
if (any(is.nan(scanl_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
if (any(is.nan(scan_rate_perID))){ scan_rate_perID[is.nan(scan_rate_perID)]=0 }
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_rate_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
#Pool results for later plotting
true_prop_results = c(true_prop_behav_perID)
true_rate_results = c(true_rate_behav_perID)
scan_rate_results = c(scan_rate_perID)
focal_rate_results = c(focal_rate_perID)
focal_prop_results = c(focal_prop_perID)
df<-data.frame(scan_rate_results, focal_rate_results,true_rate_results)
#Compute difference of rates per ID
df$diff_scan = abs(true_rate_results - scan_rate_results)
df$diff_focal = abs(true_rate_results - focal_rate_results)
sum(df$diff_scan); mean(df$diff_scan)
sum(df$diff_focal); mean(df$diff_focal)
plot_scan = density(scan_rate_results)
plot_focal = density(focal_rate_results)
plot_true = density(true_rate_results)
mean_true_rate = mean(true_rate_results)
y_max = max(c(max(plot_scan$y), max(plot_focal$y), max(plot_true$y)))
#Plot results
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_rate_behav_perID), size = 1, color="red")+ #true rate per individual
geom_density(aes(true_rate_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_rate_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_rate_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_rate_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Rate per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
colors <- c("Scan" = "blue", "Focal" = "orange", "True"="red")
ggplot(df, aes(scan_rate_results))+
geom_vline(xintercept=mean(true_prop_behav_perID), size = 1, color="red")+ #true proportion per individual
geom_density(aes(true_prop_results, color = "True"), size = 1.5)+
#annotate("text", x=mean(true_prop_behav_perID)+0.002, y=y_max+10, label="true proportion", angle=90, color='red')+
geom_density(aes(color = "Scan"), size = 1.5)+ #distribution of rate per ID from group scans
geom_vline(xintercept=median(scan_rate_results), color='blue', size = 1, linetype = 3)+ #median rate from group scans
geom_vline(xintercept=mean(scan_rate_results), color='blue', size = 1, linetype = 2)+ #mean rate from group scans
geom_density(aes(focal_prop_results, color = "Focal"), size = 1.5)+ #distribution of rate per ID from continuous focal sampling
geom_vline(xintercept=median(focal_prop_results), color='orange', size = 1, linetype = 3)+ #median rate from focal sampling
geom_vline(xintercept=mean(focal_prop_results), color='orange', size = 1, linetype = 2)+ #mean rate from focal sampling
labs(x='Proportion per ID',y='density', color="Legend")+scale_color_manual(values=colors)+ylim(c(0,y_max+40))+
theme_classic(base_size = 15)
y_max
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
##################################################################333
library(mitml)
library(lme4)
data(studentratings)
summary(studentratings)
packages()
update.packages()
update.packages(ask=FALSE)
#Load libraries
#data wrangling
library(dplyr)
library(forcats)
#Modelling
library(lme4)
library(broom.mixed)
#For plotting
library(ggplot2)
#Load data
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/")
load("BisonProximity.RData")
#Set group year list
group = c("F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V","F")
years = c(2013, 2013,2014,2014,2015,2015,2015,2015,
2016,2016,2016,2016,2017,2017,2017,
2018,2018, 2019, 2019,2021,2021,2022,2022,2022)
groupyears = paste0(group,years)
######################################
## STEP 1: CREATE IMPUTED DATASSET ###
######################################
# Extract change in proximity from each draw to create imputed data set:
imputed.data=list(); imputed.data.prepost=list(); i=1; gy=1;
imputed.density=list()
data.all<-data.frame(); density.iter=data.frame(matrix(NA, nrow=num_iter)); names(density.iter)="dens"
density.all<-data.frame();
for (gy in 1:length(groupyears)){
data_path<-'~/Documents/GitHub/Cayo-Maria-Survival/Data/Data All Cleaned/BehavioralDataFiles/'
data = read.csv(paste0(data_path,"Group",groupyears[gy],"_GroupByYear.txt")) #load meta data
data=data[data$hrs.focalfollowed>0,]
data$group = group[gy]
data$year=years[gy];
data$isPost = ifelse(years[gy]<2018, "pre","post")
data$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
data$id = as.factor(data$id)
data$sex[data$sex=="FALSE"]="F"; data$sex = as.factor(data$sex);
data$id.year = paste(data$id, data$year,sep='.')
strength<-node_strength_all[[groupyears[gy]]][i,]
degree<-node_degree_all[[groupyears[gy]]][i,]
node.id<-node_ids[[groupyears[gy]]]
data$prox.strength<-as.numeric(strength[match(data$id, node.id)])
data$prox.degree<-as.numeric(degree[match(data$id, node.id)])
data.final<-data[,c("id","sex","age","group","year","isPost","isPost.year", "id.year","prox.strength", "prox.degree")]
data.all<-rbind(data.all, data.final)
density.iter$dens=density_all[[groupyears[gy]]]
density.iter$group=group[gy]
density.iter$year=years[gy]
density.iter$isPost = ifelse(years[gy]<2018, "pre","post")
density.iter$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
density.all<-rbind(density.all, density.iter)
}
#Standerdize
density.all$std.dens <-(density.all$dens-mean(density.all$dens))/sd(density.all$dens)
data.all$std.prox.strength = (data.all$prox.strength-mean(data.all$prox.strength))/sd(data.all$prox.strength)
data.all$std.prox.degree = (data.all$prox.degree-mean(data.all$prox.degree))/sd(data.all$prox.degree)
hist(data.all$std.prox.strength)
hist(log(data.all$std.prox.strength))
#proximity_modelling_LongTermChange.R
#Model proximity network with bison - get a distribution of possible networks from the observed data
#Regression from imputed dataset to test the change in proximity over time
#C. Testard October 2022
#Load libraries
#data wrangling
library(dplyr)
library(forcats)
#Modelling
library(lme4)
library(broom.mixed)
#For plotting
library(ggplot2)
#Load data
setwd("~/Documents/GitHub/Cayo-Maria-Survival/Data/R.Data/")
load("BisonProximity.RData")
#Set group year list
group = c("F","KK","F","HH","F","V","R","KK","R","V","F","HH","F","KK","V","V","KK","S","V","F","V","TT","V","F")
years = c(2013, 2013,2014,2014,2015,2015,2015,2015,
2016,2016,2016,2016,2017,2017,2017,
2018,2018, 2019, 2019,2021,2021,2022,2022,2022)
groupyears = paste0(group,years)
######################################
## STEP 1: CREATE IMPUTED DATASSET ###
######################################
# Extract change in proximity from each draw to create imputed data set:
imputed.data=list(); imputed.data.prepost=list(); i=1; gy=1;
imputed.density=list()
for (i in 1:num_iter){
data.all<-data.frame(); density.iter=data.frame(matrix(NA, nrow=num_iter)); names(density.iter)="dens"
density.all<-data.frame();
for (gy in 1:length(groupyears)){
data_path<-'~/Documents/GitHub/Cayo-Maria-Survival/Data/Data All Cleaned/BehavioralDataFiles/'
data = read.csv(paste0(data_path,"Group",groupyears[gy],"_GroupByYear.txt")) #load meta data
data=data[data$hrs.focalfollowed>0,]
data$group = group[gy]
data$year=years[gy];
data$isPost = ifelse(years[gy]<2018, "pre","post")
data$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
data$id = as.factor(data$id)
data$sex[data$sex=="FALSE"]="F"; data$sex = as.factor(data$sex);
data$id.year = paste(data$id, data$year,sep='.')
strength<-node_strength_all[[groupyears[gy]]][i,]
degree<-node_degree_all[[groupyears[gy]]][i,]
node.id<-node_ids[[groupyears[gy]]]
data$prox.strength<-as.numeric(strength[match(data$id, node.id)])
data$prox.degree<-as.numeric(degree[match(data$id, node.id)])
data.final<-data[,c("id","sex","age","group","year","isPost","isPost.year", "id.year","prox.strength", "prox.degree")]
data.all<-rbind(data.all, data.final)
density.iter$dens=density_all[[groupyears[gy]]]
density.iter$group=group[gy]
density.iter$year=years[gy]
density.iter$isPost = ifelse(years[gy]<2018, "pre","post")
density.iter$isPost.year = ifelse(years[gy]<2018,"pre",paste(data$isPost, data$year,sep='.'))
density.all<-rbind(density.all, density.iter)
}
#Standerdize
density.all$std.dens <-(density.all$dens-mean(density.all$dens))/sd(density.all$dens)
data.all$std.prox.strength = (data.all$prox.strength-mean(data.all$prox.strength))/sd(data.all$prox.strength)
data.all$std.prox.degree = (data.all$prox.degree-mean(data.all$prox.degree))/sd(data.all$prox.degree)
#Adjust levels
data.all$group<-as.factor(data.all$group)
data.all = data.all %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
mutate(isPost.year = fct_relevel(isPost.year,
"pre", "post.2018","post.2019","post.2021", "post.2022"))
#"post.2018","pre","post.2019","post.2021", "post.2022"))
density.all = density.all %>%
mutate(isPost = fct_relevel(isPost,
"pre", "post")) %>%
mutate(isPost.year = fct_relevel(isPost.year,
"pre", "post.2018","post.2019","post.2021", "post.2022"))
#"post.2018","pre","post.2019","post.2021", "post.2022"))
imputed.data[[i]]=data.all
imputed.density[[i]]=density.all
#If only consider IDs present both pre and post
id.isPost = table(data.all$id, data.all$isPost)
id.year = table(data.all$id, data.all$year);
id_pre = row.names(as.data.frame(which(id.isPost[,"pre"]>0))); id_post = row.names(as.data.frame(which(id.isPost[,"post"]>0)))
id.PreAndPost = as.data.frame(row.names(as.data.frame(which(id.isPost[,"pre"]>0 & id.isPost[,"post"]>0)))); names(id.PreAndPost)="id"
id.PreAndPost$group = data.all$group[match(id.PreAndPost$id, data.all$id)]
table(id.PreAndPost$group)
data.prepost = subset(data.all, id %in% id.PreAndPost$id)
imputed.data.prepost[[i]]<-data.prepost
print(i)
}
#Create imputed dataset to use in frequentist survival models
imp<-miceadds::datalist2mids(imputed.data)
imp.prepost<-miceadds::datalist2mids(imputed.data.prepost)
mdl.strength.proxPrePost <- with(imp, lmer(std.prox.strength~ 1+ isPost.year +age+ sex
+(1|group) + (1|id)))
summary(mice::pool(mdl.strength.proxPrePost))
#Proximity strength
lmer(std.prox.strength~ 1+ isPost.year +age+ sex
+(1|group) + (1|id), data=data.all)
summary(mice::pool(mdl.strength.proxPrePost))
#Proximity strength
test.model = lmer(std.prox.strength~ 1+ isPost.year +age+ sex
+(1|group) + (1|id), data=data.all)
check_model(test.model)
performance::check_model(test.model)
lmer(log(std.prox.strength)~ 1+ isPost.year +age+ sex
+(1|group) + (1|id), data=data.all)
#Proximity strength
test.model = lmer(log(std.prox.strength)~ 1+ isPost.year +age+ sex
+(1|group) + (1|id), data=data.all)
performance::check_model(test.model)
which(is.nan(log(data.all$std.prox.strength))
)
log(data.all$std.prox.strength)
data.all$std.prox.strength
#Compile_TempData.R
#This script compiles all individual .csv files from the Google Drive folder
#as well as the meta data for the temperature loggers.
#Jan 2023 C. Testard
# load libraries --------
library(readr)
library(tidyverse)
library(googledrive)
library(lubridate)
library(hms)
library(ggplot2)
#Load meta data
setwd('~/Documents/GitHub/Cayo-Maria-Survival/Data/Temperature/')
marina_data = read.csv("cayo_therm_data_2018-2020.csv",sep=',')
# marina_data$year = year(marina_data$date); table(marina_data$year)
# marina_data$month = month(marina_data$date);table(marina_data$month)
# marina_data$hour = substr(marina_data$time,1,2);table(marina_data$hour)
meta_data=read.csv("Thermochron_deployment_locations_2018_2019.csv",sep = ';')
meta_data$device_address[nchar(meta_data$device_address)>16]= #Adjust misnamed temp loggers
substr(meta_data$device_address[nchar(meta_data$device_address)>16],2,17)
#remove duplicates
meta_data=meta_data[!duplicated(meta_data$device_address),]
#Load and format all individual temp records csv files
setwd('~/Documents/GitHub/Cayo-Maria-Survival/Data/Temperature/Data')
csv_files <- dir(pattern='*.csv$', recursive = T)
fi=1; all_files=data.frame()
for (fi in 1:length(csv_files)){
#Read csv file
file=read.csv(csv_files[fi],skip = 14, header = T,sep = ',')
file$file.id=fi
#Extract therm number:
file$therm.id=substr(csv_files[fi],nchar(csv_files[fi])-14,nchar(csv_files[fi])-13)
#Extract device address to merge with meta data later
device_address=read.csv(csv_files[fi]); device_address=device_address[1,1]; device_address=substr(device_address,nchar(device_address)-15,nchar(device_address))
file$device_address = device_address
#Bind file with the rest of the data
all_files = rbind(all_files, file)
print(fi)
}
#Format dataframe
names(all_files)=c("date.time","unit","value","file.id","therm.id","device_address")
all_files$date.time = as.POSIXct(all_files$date.time, format = "%m/%d/%y %I:%M:%S %p")
#IMPORTANTE NOTE: date doesnt parse properly for a certain amount of dates
# test=as.POSIXct(all_files$date.time, format = "%m/%d/%y %H:%M:%S")
# test=strptime(all_files$date.time, "%m/%d/%y %H:%M:%S")
# all_files[which(is.na(test)),]
# all_files[which(!is.na(test)),]
all_files$date = as.Date(all_files$date.time)
all_files$year = year(all_files$date.time)
#fix 2000 years. they are all 2021 files
all_files$year[all_files$year==2000]=2021
all_files$month = month(all_files$date.time)
all_files$time <- as_hms(all_files$date.time)
all_files$am.pm <-"night";
all_files$am.pm[all_files$time>as_hms("6:00:00") & all_files$time<as_hms("10:00:00")]="AM"
all_files$am.pm[all_files$time>as_hms("10:00:00") & all_files$time<as_hms("17:00:00")]="PM"
#Check device missing or name mismatch
device_names=unique(all_files$device_address)
device_names[is.na(match(device_names, meta_data$device_address))] #devices with temp data but no meta data
meta_data$device_address[is.na(match(meta_data$device_address, device_names))]#devices with no temp data but with meta data
#IMPORTANTE NOTE: Need to update for 2022 change in location / new loggers
#Merge location and vegetation status with temp data
temp_data=all_files
temp_data[,c("location","vegetation")] = meta_data[match(all_files$device_address,meta_data$device_address), c("location","vegetation")]
#temp_data[,c("location","vegetation")] = as.factor(temp_data[,c("location","vegetation")])
temp_data$gross_location = "Big cayo";
temp_data$gross_location[temp_data$location=="SCU"|temp_data$location=="SCL"]="Small cayo"
#Format temp data for plot
#Remove missing data
temp_data_final = temp_data[rowSums(is.na(temp_data)) == 0, ]
#Remove 2022 data for now (location may have changed)
temp_data_final = temp_data_final[temp_data_final$year<2022,]
#Remove night time data
temp_data_final = temp_data_final[temp_data_final$am.pm!="night",]
#Lump de-vegetated areas
temp_data_final$is.shaded=ifelse(temp_data_final$vegetation == "Vegetated","Shaded","Exposed")
#Find how much of the time during the day temperatures exceed 40C
temp_data_day_exposed = temp_data_final[temp_data_final$is.shaded=="Exposed" &
temp_data_final$time > "10:00:00",]
length(which(temp_data_day_exposed$value>40))/nrow(temp_data_day_exposed)
temp_data_final$time
View(temp_data_day_exposed)
#Find how much of the time during the day temperatures exceed 40C
temp_data_day_exposed = temp_data_final[temp_data_final$is.shaded=="Exposed" &
temp_data_final$time > as.hms("10:00:00"),]
#Find how much of the time during the day temperatures exceed 40C
temp_data_day_exposed = temp_data_final[temp_data_final$is.shaded=="Exposed" &
temp_data_final$time > as_hms("10:00:00"),]
length(which(temp_data_day_exposed$value>40))/nrow(temp_data_day_exposed)
